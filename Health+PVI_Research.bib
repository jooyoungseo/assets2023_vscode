@article{AccessibilityMobileHealth,
  title = {The {{Accessibility}} of {{Mobile Health Sensors}} for {{Blind Users}}},
  file = {C\:\\Users\\jseo1005\\Zotero\\storage\\6MRJ2AKJ\\The Accessibility of Mobile Health Sensors for Bli.pdf}
}

@article{afshinHealthEffectsDietary2019,
  title = {Health Effects of Dietary Risks in 195 Countries, 1990\textendash 2017: {{A}} Systematic Analysis for the {{Global Burden}} of {{Disease Study}} 2017},
  author = {Afshin, Ashkan and Sur, Patrick John and Fay, Kairsten A. and Cornaby, Leslie and Ferrara, Giannina and Salama, Joseph S. and Mullany, Erin C. and Abate, Kalkidan Hassen and Abbafati, Cristiana and Abebe, Zegeye},
  year = {2019},
  journal = {The Lancet},
  volume = {393},
  number = {10184},
  pages = {1958--1972},
  publisher = {{Elsevier}},
  isbn = {0140-6736}
}

@misc{AmericansDisabilitiesAct,
  title = {The {{Americans}} with {{Disabilities Act}}},
  journal = {ADA.gov},
  urldate = {2022-12-14},
  abstract = {Disability rights are civil rights. From voting to parking, the ADA is a law that protects people with disabilities in many areas of public life.},
  howpublished = {https://www.ada.gov/},
  langid = {english},
  file = {C\:\\Users\\jseo1005\\Zotero\\storage\\L7AVQ4ZW\\www.ada.gov.html}
}

@article{anirUsersPerceptionsOpportunities2008,
  title = {The Users Perceptions and Opportunities in {{Malaysia}} in Introducing {{RFID}} System for {{Halal}} Food Tracking},
  author = {Anir, Norman Azah and Nizam, {\relax MNMH} and Masliyana, Azmi},
  year = {2008},
  journal = {WSEAS Transactions on information science and applications},
  volume = {5},
  number = {5},
  pages = {843--852},
  publisher = {{World Scientific and Engineering Academy and Society (WSEAS) Stevens Point~\ldots}},
  isbn = {1790-0832}
}

@misc{AteApp,
  title = {The {{Ate}} App},
  urldate = {2022-12-12},
  abstract = {An easy to use food journaling app designed to help you establish and maintain a healthy eating habit.},
  howpublished = {https://youate.com/},
  langid = {english},
  file = {C\:\\Users\\jseo1005\\Zotero\\storage\\N9Z9VBTB\\youate.com.html}
}

@inproceedings{balataBlindCameraCentralGoldenratio2015,
  title = {{{BlindCamera}}: {{Central}} and {{Golden-ratio Composition}} for {{Blind Photographers}}},
  shorttitle = {{{BlindCamera}}},
  booktitle = {Proceedings of the {{Mulitimedia}}, {{Interaction}}, {{Design}} and {{Innnovation}}},
  author = {Balata, Jan and Mikovec, Zdenek and Neoproud, Lukas},
  year = {2015},
  month = jun,
  series = {{{MIDI}} '15},
  pages = {1--8},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2814464.2814472},
  urldate = {2022-12-15},
  abstract = {Even though the photography is a visual medium, visually impaired people use their smartphone cameras to take photos. We present a smartphone application, which helps visually impaired people to properly aim while taking portrait photographs. We have implemented an aiming assistance algorithm for two compositions (central, golden-ratio) and two feedback modalities (voice, vibration). In a study with 12 blindfolded participants we compared four combinations (composition \texttimes{} feedback modality) in aiming time, interaction, completion time, aesthetic quality of photos, and comfort of use. The results show that the combination of golden ratio composition with voice feedback produced the best aiming time (5.53 seconds) and completion time (8.33 seconds), the best aesthetic quality of photos, and was perceived as most comfortable. The interaction time of double click was 2.92 seconds on average and there was no significant difference between the four combinations.},
  isbn = {978-1-4503-3601-7},
  keywords = {focalization,photography,user study,Visually impaired},
  file = {C\:\\Users\\jseo1005\\Zotero\\storage\\QUJ8YNIA\\Balata et al. - 2015 - BlindCamera Central and Golden-ratio Composition .pdf}
}

@inproceedings{ballantyneStudyAccessibilityGuidelines2018,
  title = {Study of {{Accessibility Guidelines}} of {{Mobile Applications}}},
  booktitle = {Proceedings of the 17th {{International Conference}} on {{Mobile}} and {{Ubiquitous Multimedia}}},
  author = {Ballantyne, Mars and Jha, Archit and Jacobsen, Anna and Hawker, J. Scott and {El-Glaly}, Yasmine N.},
  year = {2018},
  month = nov,
  pages = {305--315},
  publisher = {{ACM}},
  address = {{Cairo Egypt}},
  doi = {10.1145/3282894.3282921},
  urldate = {2022-10-12},
  abstract = {With the increased ubiquity of mobile devices around the world, it is imperative to ensure that these devices and their applications (apps) are accessible to users with disabilities. Although design style guides are undergoing a paradigm shift with the promotion of `mobile-first' ideology, we have yet to witness a concrete step being taken towards the establishment of universal guidelines for mobile app accessibility. To address this issue, we compile an exhaustive list of guidelines to gauge mobile app accessibility. We present a mobile-specific framework to categorize the guidelines. We then underline the importance of these clearly defined guidelines by putting the most popular 25 apps from the Google Play Store under their lens. The results indicate low rates of violations of accessibility guidelines at the system level, and a high rate of violations at design and content levels. We highlight the most and the least violated guidelines. We discuss in detail the overall accessibility of evaluated apps and identified patterns in violations of established rules.},
  isbn = {978-1-4503-6594-9},
  langid = {english},
  file = {C\:\\Users\\jseo1005\\Zotero\\storage\\7XKJ45JV\\Ballantyne et al. - 2018 - Study of Accessibility Guidelines of Mobile Applic.pdf}
}

@inproceedings{ballantyneStudyAccessibilityGuidelines2018a,
  title = {Study of Accessibility Guidelines of Mobile Applications},
  booktitle = {Proceedings of the 17th {{International Conference}} on {{Mobile}} and {{Ubiquitous Multimedia}}},
  author = {Ballantyne, Mars and Jha, Archit and Jacobsen, Anna and Hawker, J. Scott and {El-Glaly}, Yasmine N.},
  year = {2018},
  month = nov,
  series = {{{MUM}} 2018},
  pages = {305--315},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3282894.3282921},
  urldate = {2022-12-07},
  abstract = {With the increased ubiquity of mobile devices around the world, it is imperative to ensure that these devices and their applications (apps) are accessible to users with disabilities. Although design style guides are undergoing a paradigm shift with the promotion of 'mobile-first' ideology, we have yet to witness a concrete step being taken towards the establishment of universal guidelines for mobile app accessibility. To address this issue, we compile an exhaustive list of guidelines to gauge mobile app accessibility. We present a mobile-specific framework to categorize the guidelines. We then underline the importance of these clearly defined guidelines by putting the most popular 25 apps from the Google Play Store under their lens. The results indicate low rates of violations of accessibility guidelines at the system level, and a high rate of violations at design and content levels. We highlight the most and the least violated guidelines. We discuss in detail the overall accessibility of evaluated apps and identified patterns in violations of established rules.},
  isbn = {978-1-4503-6594-9},
  keywords = {Accessibility,applications,evaluation,heuristics,mobile,people with disabilities},
  file = {C\:\\Users\\jseo1005\\Zotero\\storage\\R3QFNMC2\\Ballantyne et al. - 2018 - Study of Accessibility Guidelines of Mobile Applic.pdf}
}

@misc{BeMyEyes,
  title = {Be {{My Eyes}}},
  urldate = {2022-12-08},
  abstract = {Whether you need a pair of sharp eyes or have some sight to lend, Be My Eyes is a simple, free tool to help people see the world better, together.},
  howpublished = {https://www.bemyeyes.com/},
  langid = {english},
  file = {C\:\\Users\\jseo1005\\Zotero\\storage\\CT2Q3UG9\\www.bemyeyes.com.html}
}

@incollection{bentleyEvaluatingMobileVisualizations2021,
  title = {Evaluating {{Mobile Visualizations}}},
  booktitle = {Mobile {{Data Visualization}}},
  author = {Bentley, Frank and Choe, Eun Kyoung and Mamykina, Lena and Stasko, John and Irani, Pourang},
  year = {2021},
  month = dec,
  pages = {177--208},
  publisher = {{Chapman and Hall/CRC}},
  doi = {10.1201/9781003090823-6},
  urldate = {2022-12-08},
  abstract = {This chapter discusses the special challenges of evaluating mobile visualizations. Many research goals can be addressed by an evaluation study including validating rapid perception of differences in data or examining the long-term use and impact of visualizations. Different methods, time-scales of research, and participant recruitment strategies are needed depending on the questions that one wants to answer. This chapter explores the literature, discussing a variety of goals and evaluation approaches, highlighting best practices and making recommendations for future approaches to evaluating mobile visualizations.},
  isbn = {978-1-00-309082-3},
  langid = {english},
  file = {C\:\\Users\\jseo1005\\Zotero\\storage\\DZA3SBZP\\evaluating-mobile-visualizations-frank-bentley-eun-kyoung-choe-lena-mamykina-john-stasko-pouran.html}
}

@article{biAuracleDetectingEating2018,
  title = {Auracle: {{Detecting}} Eating Episodes with an Ear-Mounted Sensor},
  author = {Bi, Shengjie and Wang, Tao and Tobias, Nicole and Nordrum, Josephine and Wang, Shang and Halvorsen, George and Sen, Sougata and Peterson, Ronald and Odame, Kofi and Caine, Kelly},
  year = {2018},
  journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
  volume = {2},
  number = {3},
  pages = {1--27},
  publisher = {{ACM New York, NY, USA}},
  isbn = {2474-9567}
}

@inproceedings{bighamVizWizLocateItEnabling2010,
  title = {{{VizWiz}}::{{LocateIt}} - Enabling Blind People to Locate Objects in Their Environment},
  shorttitle = {{{VizWiz}}},
  booktitle = {2010 {{IEEE Computer Society Conference}} on {{Computer Vision}} and {{Pattern Recognition}} - {{Workshops}}},
  author = {Bigham, Jeffrey P. and Jayant, Chandrika and Miller, Andrew and White, Brandyn and Yeh, Tom},
  year = {2010},
  month = jun,
  pages = {65--72},
  issn = {2160-7516},
  doi = {10.1109/CVPRW.2010.5543821},
  abstract = {Blind people face a number of challenges when interacting with their environments because so much information is encoded visually. Text is pervasively used to label objects, colors carry special significance, and items can easily become lost in surroundings that cannot be quickly scanned. Many tools seek to help blind people solve these problems by enabling them to query for additional information, such as color or text shown on the object. In this paper we argue that many useful problems may be better solved by directly modeling them as search problems, and present a solution called VizWiz::LocateIt that directly supports this type of interaction. VizWiz::LocateIt enables blind people to take a picture and ask for assistance in finding a specific object. The request is first forwarded to remote workers who outline the object, enabling efficient and accurate automatic computer vision to guide users interactively from their existing cellphones. A two-stage algorithm is presented that uses this information to guide users to the appropriate object interactively from their phone.},
  keywords = {Cellular phones,Computer vision,DVD,Educational institutions,Elevators,Face detection,Humans,Recruitment,Search problems,Web pages},
  file = {C\:\\Users\\jseo1005\\Zotero\\storage\\DG95FAAS\\Bigham et al. - 2010 - VizWizLocateIt - enabling blind people to locate.pdf;C\:\\Users\\jseo1005\\Zotero\\storage\\9WL5AAIJ\\5543821.html}
}

@article{bilykFoodExperiencesEating2009,
  title = {Food Experiences and Eating Patterns of Visually Impaired and Blind People},
  author = {Bilyk, Marie Claire and Sontrop, Jessica M. and Chapman, Gwen E. and Barr, Susan I. and Mamer, Linda},
  year = {2009},
  journal = {Canadian Journal of Dietetic practice and research},
  volume = {70},
  number = {1},
  pages = {13--18},
  publisher = {{Dietitians of Canada}},
  isbn = {1486-3847}
}

@misc{Bitesnap,
  title = {Bitesnap},
  urldate = {2022-12-12},
  abstract = {Bitesnap is the easier way to track what you eat. Count calories and nutrients just by taking a picture. Bitesnap recognizes the foods in your meals, saving you time and making it simple to build healthy eating habits.},
  howpublished = {https://getbitesnap.com},
  langid = {english},
  file = {C\:\\Users\\jseo1005\\Zotero\\storage\\CVD3BALZ\\getbitesnap.com.html}
}

@article{blairOneNoteMealPhotobased2018,
  title = {{{OneNote Meal}}: {{A}} Photo-Based Diary Study for Reflective Meal Tracking},
  shorttitle = {{{OneNote Meal}}},
  author = {Blair, Johnna and Luo, Yuhan and Ma, Ning F. and Lee, Sooyeon and Choe, Eun Kyoung},
  year = {2018},
  month = dec,
  journal = {AMIA Annual Symposium Proceedings},
  volume = {2018},
  pages = {252--261},
  issn = {1942-597X},
  urldate = {2022-12-12},
  abstract = {When a self-monitoring tool is used to enhance behavior awareness, the tool should afford reflection by design. This work examines the ``valence of meal'' (i.e., healthy versus unhealthy meal) as a means to support reflection on a person's diet in photo-based meal tracking. To study the effect of imposing valence on meal tracking, we designed two conditions\textemdash one focusing on capturing healthy meals, the other capturing unhealthy meals\textemdash and conducted a between-subjects diary study with 22 college students over four weeks. According to their group assignment, participants tracked only healthy or unhealthy meals by taking photos and rationalizing in texts why their meals were particularly healthy or unhealthy. We found that participants in both groups became more aware of their diet, but the valence of meal influenced them differently regarding their meal assessment, self-reflection, and food choice intention. We discuss ways to leverage valence in designing reflective meal tracking systems.},
  pmcid = {PMC6371351},
  pmid = {30815063},
  file = {C\:\\Users\\jseo1005\\Zotero\\storage\\3T7KGJRR\\Blair et al. - 2018 - OneNote Meal A Photo-Based Diary Study for Reflec.pdf}
}

@misc{BlindnessStatisticsNational,
  title = {Blindness {{Statistics}} | {{National Federation}} of the {{Blind}}},
  urldate = {2022-12-02},
  abstract = {There are several ways to define blindness.},
  howpublished = {https://nfb.org/resources/blindness-statistics},
  langid = {english}
}

@article{blockReviewValidationsDietary1982,
  title = {A Review of Validations of Dietary Assessment Methods},
  author = {Block, G.},
  year = {1982},
  month = apr,
  journal = {American Journal of Epidemiology},
  volume = {115},
  number = {4},
  pages = {492--505},
  issn = {0002-9262},
  doi = {10.1093/oxfordjournals.aje.a113331},
  langid = {english},
  pmid = {7041631},
  keywords = {Data Collection,Diet Surveys,Epidemiologic Methods,Evaluation Studies as Topic,Humans,Mental Recall,Nutrition Surveys,Surveys and Questionnaires}
}

@article{bolandENutritionTheNextDimension2019,
  title = {{{eNutrition-The}} next Dimension for {{eHealth}}?},
  author = {Boland, Mike and Bronlund, John},
  year = {2019},
  journal = {Trends in Food Science \& Technology},
  volume = {91},
  pages = {634--639},
  publisher = {{Elsevier}},
  isbn = {0924-2244}
}

@article{borgesDevelopmentValidationProtocols2021,
  title = {Development and Validation of Protocols for Photographed Food Record by Visually Impaired People},
  author = {Borges, Tha{\'i}s Lima Dias and {de Lima}, Marcos Felipe Silva and Lima, Severina Carla Vieira Cunha and Bagni, Ursula Viana},
  year = {2021},
  journal = {Public Health Nutrition},
  volume = {24},
  number = {5},
  pages = {903--913},
  publisher = {{Cambridge University Press}},
  isbn = {1368-9800}
}

@article{bourneTrendsPrevalenceBlindness2021,
  title = {Trends in Prevalence of Blindness and Distance and near Vision Impairment over 30 Years: {{An}} Analysis for the {{Global Burden}} of {{Disease Study}}},
  author = {Bourne, Rupert and Steinmetz, Jaimie D. and Flaxman, Seth and Briant, Paul Svitil and Taylor, Hugh R. and Resnikoff, Serge and Casson, Robert James and Abdoli, Amir and {Abu-Gharbieh}, Eman and Afshin, Ashkan},
  year = {2021},
  journal = {The Lancet global health},
  volume = {9},
  number = {2},
  pages = {e130-e143},
  publisher = {{Elsevier}},
  isbn = {2214-109X}
}

@inproceedings{caineLocalStandardsSample2016,
  title = {Local Standards for Sample Size at {{CHI}}},
  booktitle = {Proceedings of the 2016 {{CHI}} Conference on Human Factors in Computing Systems},
  author = {Caine, Kelly},
  year = {2016},
  pages = {981--992}
}

@article{capella-mcdonnallNeedHealthPromotion2007,
  title = {The {{Need}} for {{Health Promotion}} for {{Adults}} Who Are {{Visually Impaired}}},
  author = {{Capella-McDonnall}, Michele},
  year = {2007},
  month = mar,
  journal = {Journal of Visual Impairment \& Blindness},
  volume = {101},
  number = {3},
  pages = {133--145},
  issn = {0145-482X, 1559-1476},
  doi = {10.1177/0145482X0710100302},
  urldate = {2022-05-13},
  abstract = {Health promotion interventions for adults who are visually impaired have received little attention. This article reports what is currently known about the health, overweight and obesity, and levels of physical activity reported by these adults. Conclusions about the need for health promotion activities based on this information are provided, and suggestions for implementing these activities or interventions are offered.},
  langid = {english},
  file = {C\:\\Users\\jseo1005\\Zotero\\storage\\K2XL8PVI\\Capella-McDonnall - 2007 - The Need for Health Promotion for Adults who are V.pdf}
}

@article{choeSemiautomatedTrackingBalanced2017,
  title = {Semi-Automated Tracking: {{A}} Balanced Approach for Self-Monitoring Applications},
  shorttitle = {Semi-{{Automated Tracking}}},
  author = {Choe, Eun Kyoung and Abdullah, Saeed and Rabbi, Mashfiqui and Thomaz, Edison and Epstein, Daniel A. and Cordeiro, Felicia and Kay, Matthew and Abowd, Gregory D. and Choudhury, Tanzeem and Fogarty, James and Lee, Bongshin and Matthews, Mark and Kientz, Julie A.},
  year = {2017},
  month = jan,
  journal = {IEEE Pervasive Computing},
  volume = {16},
  number = {1},
  pages = {74--84},
  issn = {1558-2590},
  doi = {10.1109/MPRV.2017.18},
  abstract = {The authors present an approach for designing self-monitoring technology called "semi-automated tracking," which combines both manual and automated data collection methods. Through this approach, they aim to lower the capture burdens, collect data that is typically hard to track automatically, and promote awareness to help people achieve their self-monitoring goals. They first specify three design considerations for semi-automated tracking: data capture feasibility, the purpose of self-monitoring, and the motivation level. They then provide examples of semi-automated tracking applications in the domains of sleep, mood, and food tracking to demonstrate strategies they developed to find the right balance between manual tracking and automated tracking, combining each of their benefits while minimizing their associated limitations.},
  keywords = {bioinformatics,data analysis,Data collection,food tracking,healthcare,Informatics,Insulation life,Internet of things,Internet of Things,Medical devices,mobile,Monitoring,mood tracking,Mood tracking,personal informatics,pervasive computing,Pervasive computing,self-monitoring,semi-automated tracking,Sensors,sleep tracking},
  file = {C\:\\Users\\jseo1005\\Zotero\\storage\\LTCAT2LG\\Choe et al. - 2017 - Semi-Automated Tracking A Balanced Approach for S.pdf;C\:\\Users\\jseo1005\\Zotero\\storage\\M4XLATN7\\7807194.html}
}

@unpublished{choiCloserLookBlind,
  title = {A Closer Look at Blind Young Adults' Health Behaviors: {{A}} Multiple Case Study ([{{Manuscript}} Submitted for Publication])},
  author = {Choi, Soyoung and Lim, Areum and Lee, Hyangkyu},
  langid = {english}
}

@article{chungIdentifyingPlanningIndividualized2019,
  title = {Identifying and Planning for Individualized Change: {{Patient-provider}} Collaboration Using Lightweight Food Diaries in Healthy Eating and Irritable Bowel Syndrome},
  shorttitle = {Identifying and {{Planning}} for {{Individualized Change}}},
  author = {Chung, Chia-Fang and Wang, Qiaosi and Schroeder, Jessica and Cole, Allison and Zia, Jasmine and Fogarty, James and Munson, Sean A.},
  year = {2019},
  month = mar,
  journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
  volume = {3},
  number = {1},
  pages = {7:1--7:27},
  doi = {10.1145/3314394},
  urldate = {2022-12-14},
  abstract = {Identifying and planning strategies that support a healthy lifestyle or manage a chronic disease often require patient-provider collaboration. For example, people with healthy eating goals often share everyday food, exercise, or sleep data with health coaches or nutritionists to find opportunities for change, and patients with irritable bowel syndrome (IBS) often gather food and symptom data as part of working with providers to diagnose and manage symptoms. However, a lack of effective support often prevents health experts from reviewing large amounts of data in time-constrained visits, prevents focusing on individual goals, and prevents generating correct, individualized, and actionable recommendations. To examine how to design photo-based diaries to help people and health experts exchange knowledge and focus on collaboration goals when reviewing the data together, we designed and developed Foodprint, a photo-based food diary. Foodprint includes three components: (1) A mobile app supporting lightweight data collection, (2) a web app with photo-based visualization and quantitative visualizations supporting collaborative reflection, and (3) a pre-visit note communicating an individual's expectations and questions to experts. We deployed Foodprint in two studies: (1) with 17 people with healthy eating goals and 7 health experts, and (2) with 16 IBS patients and 8 health experts. Building upon the lens of boundary negotiating artifacts and findings from two field studies, our research contributes design principles to (1) prepare individuals to collect data relevant to their health goals and for collaboration, (2) help health experts focus on an individual's eating context, experiences, and goals in collaborative review, and (3) support individuals and experts to develop individualized, actionable plans and strategies.},
  keywords = {collaboration,food,patient-generated health data,patient-provider collaboration,personal informatics,Self-tracking},
  file = {C\:\\Users\\jseo1005\\Zotero\\storage\\GFS6B7NS\\Chung et al. - 2019 - Identifying and Planning for Individualized Change.pdf}
}

@inproceedings{chungWhenPersonalTracking2017,
  title = {When Personal Tracking Becomes Social: {{Examining}} the Use of {{Instagram}} for Healthy Eating},
  booktitle = {Proceedings of the 2017 {{CHI Conference}} on Human Factors in Computing Systems},
  author = {Chung, Chia-Fang and Agapie, Elena and Schroeder, Jessica and Mishra, Sonali and Fogarty, James and Munson, Sean A.},
  year = {2017},
  pages = {1674--1687}
}

@inproceedings{clegg-vinellInvestigatingAppropriatenessRelevance2014,
  title = {Investigating the Appropriateness and Relevance of Mobile Web Accessibility Guidelines},
  booktitle = {Proceedings of the 11th {{Web}} for {{All Conference}} on - {{W4A}} '14},
  author = {{Clegg-Vinell}, Raphael and Bailey, Christopher and Gkatzidou, Voula},
  year = {2014},
  pages = {1--4},
  publisher = {{ACM Press}},
  address = {{Seoul, Korea}},
  doi = {10.1145/2596695.2596717},
  urldate = {2022-02-20},
  abstract = {The Web Accessibility Initiative (WAI) of the World Wide Web Consortium (W3C) develop and maintain guidelines for making the web more accessible to people with disabilities. WCAG 2.0 and the MWBP 1.0 are internationally regarded as the industry standard guidelines for web accessibility. Mobile testing sessions conducted by AbilityNet document issues raised by users in a report format, relating issues to guidelines wherever possible. This paper presents the results of a preliminary investigation that examines how effectively and easily these issues can be related by experts to the guidelines provided by WCAG 2.0 and MWBP 1.0.},
  isbn = {978-1-4503-2651-3},
  langid = {english},
  file = {C\:\\Users\\jseo1005\\Zotero\\storage\\6AJMN9GR\\Clegg-Vinell et al. - 2014 - Investigating the appropriateness and relevance of.pdf}
}

@misc{CognitiveSpeechServices,
  title = {Cognitive {{Speech Services}} \textendash{} {{Text}}/{{Speech Analysis}} | {{Microsoft Azure}}},
  urldate = {2022-12-14},
  abstract = {Explore speech services from Microsoft Azure that include speech recognition, text to speech, speech translation, voice-enabled app features, and more.},
  howpublished = {https://azure.microsoft.com/en-us/products/cognitive-services/speech-services/},
  langid = {english},
  file = {C\:\\Users\\jseo1005\\Zotero\\storage\\QL3CLH8J\\speech-services.html}
}

@misc{CommitmentAccessibleProducts,
  title = {Commitment to {{Accessible Products}}},
  journal = {Qualtrics},
  urldate = {2022-12-15},
  abstract = {At Qualtrics, we believe that our products should reflect the diversity of all our customers. Part of hearing every voice is building products that are accessible to all individuals, including the more than 1 billion people who have a disability.},
  howpublished = {https://www.qualtrics.com/commitment-to-accessibility/},
  langid = {english},
  file = {C\:\\Users\\jseo1005\\Zotero\\storage\\RMJ2KWX9\\commitment-to-accessibility.html}
}

@inproceedings{cordeiroBarriersNegativeNudges2015,
  title = {Barriers and Negative Nudges: {{Exploring}} Challenges in Food Journaling},
  booktitle = {Proceedings of the 33rd Annual {{ACM}} Conference on Human Factors in Computing Systems},
  author = {Cordeiro, Felicia and Epstein, Daniel A. and Thomaz, Edison and Bales, Elizabeth and Jagannathan, Arvind K. and Abowd, Gregory D. and Fogarty, James},
  year = {2015},
  pages = {1159--1162}
}

@inproceedings{cordeiroRethinkingMobileFood2015,
  title = {Rethinking the Mobile Food Journal: {{Exploring}} Opportunities for Lightweight Photo-Based Capture},
  shorttitle = {Rethinking the {{Mobile Food Journal}}},
  booktitle = {Proceedings of the 33rd {{Annual ACM Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Cordeiro, Felicia and Bales, Elizabeth and Cherry, Erin and Fogarty, James},
  year = {2015},
  month = apr,
  series = {{{CHI}} '15},
  pages = {3207--3216},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2702123.2702154},
  urldate = {2022-12-12},
  abstract = {Food choices are among the most frequent and important health decisions in everyday life, but remain notoriously difficult to capture. This work examines opportunities for lightweight photo-based capture in mobile food journals. We first report on a survey of 257 people, examining how they define healthy eating, their experiences and challenges with existing food journaling methods, and their ability to interpret nutritional information that can be captured in a food journal. We then report on interviews and a field study with 27 participants using a lightweight, photo-based food journal for between 4 to 8 weeks. We discuss mismatches between motivations and current designs, challenges of current approaches to food journaling, and opportunities for photos as an alternative to the pervasive but often inappropriate emphasis on quantitative tracking in mobile food journals.},
  isbn = {978-1-4503-3145-6},
  keywords = {food journals,personal informatics,photos,self-tracking},
  file = {C\:\\Users\\jseo1005\\Zotero\\storage\\XBQ7CLIP\\Cordeiro et al. - 2015 - Rethinking the Mobile Food Journal Exploring Oppo.pdf}
}

@article{crawfordComparativeAdvantage3day1994,
  title = {Comparative Advantage of 3-Day Food Records over 24-Hour Recall and 5-Day Food Frequency Validated by Observation of 9- and 10-Year-Old Girls},
  author = {Crawford, P. B. and Obarzanek, E. and Morrison, J. and Sabry, Z. I.},
  year = {1994},
  month = jun,
  journal = {Journal of the American Dietetic Association},
  volume = {94},
  number = {6},
  pages = {626--630},
  issn = {0002-8223},
  doi = {10.1016/0002-8223(94)90158-9},
  abstract = {OBJECTIVE: The validity of the 24-hour recall, 3-day food record, and 5-day food frequency was assessed to decide on a dietary assessment method for the National Heart, Lung, and Blood Institute (NHLBI) Growth and Health Study. DESIGN: All subjects were assigned to one of three dietary assessment methods. Unobtrusive observers recorded types and amounts of foods eaten during lunch, and these were compared with the foods reported by the girls in the study. SETTING: School lunchrooms in California and Ohio. SUBJECTS: 58 girls, aged 9 and 10 years. MAIN OUTCOME MEASURES: Reporting errors for dietary assessment methods. STATISTICAL ANALYSES PERFORMED: Descriptive statistics, matched pair t tests, and Spearman correlation coefficients. RESULTS: Comparison of the intakes of energy and selected macronutrients showed different ranges of, and median percentage absolute errors for, each dietary assessment method. Percentage absolute errors ranged between 20 and 33 for the 5-day food frequency method; 19 and 39 for the 24-hour recall; and 12 and 22 for the 3-day food record. The proportion of missing foods (ie, observed food items not reported) and phantom foods (ie, reported food items not observed) by each method were 46\% and 40\%, respectively, for the 5-day food frequency; 30\% and 33\%, respectively, for the 24-hour recall; and 25\% and 10\%, respectively, for the 3-day food record. APPLICATIONS/CONCLUSIONS: Errors in food reporting and quantification can vary with the type of dietary methodology. Agreement between observed and reported intakes from 3-day food records made it the best overall choice. On this basis, it was selected as the method of assessment for the NHLBI Growth and Health Study.},
  langid = {english},
  pmid = {8195550},
  keywords = {Bias,Child,Cohort Studies,Diet Records,Eating,Evaluation Studies as Topic,Female,Humans,Interviews as Topic,Mental Recall,Prospective Studies,Random Allocation,Reproducibility of Results,Surveys and Questionnaires}
}

@article{crewsAssociationHealthrelatedQuality2016,
  title = {The Association of Health-Related Quality of Life with Severity of Visual Impairment among People Aged 40\textendash 64 Years: {{Findings}} from the 2006\textendash 2010 Behavioral Risk Factor Surveillance System},
  author = {Crews, John E. and Chou, Chiu-Fang and Zack, Matthew M. and Zhang, Xinzhi and Bullard, Kai McKeever and Morse, Alan R. and Saaddine, Jinan B.},
  year = {2016},
  journal = {Ophthalmic epidemiology},
  volume = {23},
  number = {3},
  pages = {145--153},
  publisher = {{Taylor \& Francis}},
  isbn = {0928-6586}
}

@article{crewsFallsPersonsAged2016,
  title = {Falls among Persons Aged{$\geq$} 65 Years with and without Severe Vision Impairment\textemdash{{United States}}, 2014},
  author = {Crews, John E. and Chou, Chiu-Fung and Stevens, Judy A. and Saaddine, Jinan B.},
  year = {2016},
  journal = {Morbidity and Mortality Weekly Report},
  volume = {65},
  number = {17},
  pages = {433--437},
  publisher = {{JSTOR}},
  isbn = {0149-2195}
}

@article{dabbsUsercenteredDesignInteractive2009,
  title = {User-Centered Design and Interactive Health Technologies for Patients},
  author = {Dabbs, Annette De Vito and Myers, Brad A. and Mc Curry, Kenneth R. and {Dunbar-Jacob}, Jacqueline and Hawkins, Robert P. and Begey, Alex and Dew, Mary Amanda},
  year = {2009},
  journal = {Computers, informatics, nursing: CIN},
  volume = {27},
  number = {3},
  pages = {175},
  publisher = {{NIH Public Access}}
}

@article{deruyterMobileHealthApps2018,
  title = {Mobile {{Health Apps}} and {{Needs}} of {{People}} with {{Disabilities}}: {{A National Survey}}},
  author = {DeRuyter, Frank and Jones, Michael L and Morris, John T},
  year = {2018},
  pages = {13},
  abstract = {This report summarizes data from a national survey on the experiences, needs and potential solutions for mHealth technology by people with physical, cognitive, sensory and emotional disabilities. Convenience sampling was used to draw a sample of 377 adults with disabilities. Data were collected from February to August 2017. The survey was conducted by the Rehabilitation Engineering Research Center for Community Living, Health and Function (LiveWell RERC). The survey instrument includes items on user experiences and needs for a wide range of mHealth solutions. This paper focuses on mHealth apps: 1) types of health/wellness mobile apps currently used by people with disabilities; 2) satisfaction levels with the use of health/wellness apps; 3) ease/difficulty in finding usable and effective health/wellness apps; 4) interest in an online repository of information/reviews of mHealth apps; 5) specific problems or challenges using health/wellness apps; and 6) ``wish list'' for health/wellness apps that currently do not exist.},
  langid = {english},
  file = {C\:\\Users\\jseo1005\\Zotero\\storage\\YNAQYTZX\\DeRuyter et al. - 2018 - Mobile Health Apps and Needs of People with Disabi.pdf}
}

@misc{DesigningToolsHighQuality,
  title = {Designing {{Tools}} for {{High-Quality Alt Text Authoring}} | {{Proceedings}} of the 23rd {{International ACM SIGACCESS Conference}} on {{Computers}} and {{Accessibility}}},
  urldate = {2022-12-14},
  howpublished = {https://dl.acm.org/doi/abs/10.1145/3441852.3471207}
}

@article{globeSelfreportedComorbiditiesVisual2005,
  title = {Self-Reported Comorbidities and Visual Function in a Population-Based Study: {{The Los Angeles Latino Eye Study}}},
  author = {Globe, Denise R. and Varma, Rohit and Torres, Mina and Wu, Joanne and Klein, Ronald and Azen, Stanley P. and Group, Los Angeles Latino Eye Study},
  year = {2005},
  journal = {Archives of Ophthalmology},
  volume = {123},
  number = {6},
  pages = {815--821},
  publisher = {{American Medical Association}},
  isbn = {0003-9950}
}

@article{harrisInstituteMedicineNew2012,
  title = {The {{Institute}} of {{Medicine}}'s New Report on Living Well with Chronic Illness},
  author = {Harris, Jeffrey R. and Wallace, Robert B.},
  year = {2012},
  journal = {Preventing chronic disease},
  volume = {9},
  publisher = {{Centers for Disease Control and Prevention}}
}

@article{hartleyCircadianRhythmDisturbances2018,
  title = {Circadian Rhythm Disturbances in the Blind},
  author = {Hartley, Sarah and Dauvilliers, Yves and {Quera-Salva}, Maria-Antonia},
  year = {2018},
  journal = {Current neurology and neuroscience reports},
  volume = {18},
  number = {10},
  pages = {1--8},
  publisher = {{Springer}},
  isbn = {1534-6293}
}

@misc{HelpPeopleWho,
  title = {Help {{People}} Who Are {{Blind}} or {{Partially Sighted}}},
  journal = {OrCam},
  urldate = {2022-12-15},
  abstract = {OrCam is all about using technology to help empower blind and partially sighted people live their lives with a high degree of independence.},
  howpublished = {https://www.orcam.com/en/},
  langid = {american},
  file = {C\:\\Users\\jseo1005\\Zotero\\storage\\MKQHESXK\\en.html}
}

@article{houAssociationVisualImpairment2022,
  title = {Association between Visual Impairment and Health Care Use},
  author = {Hou, Chiun-Ho and Pu, Christy},
  year = {2022},
  journal = {American Journal of Ophthalmology},
  volume = {234},
  pages = {166--173},
  publisher = {{Elsevier}},
  isbn = {0002-9394}
}

@inproceedings{ianliStagebasedModelPersonal2010,
  title = {A Stage-Based Model of Personal Informatics Systems},
  booktitle = {Proceedings of the {{SIGCHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {{Ian Li} and {Anind Dey} and {Forlizzi}},
  year = {2010},
  month = apr,
  series = {{{CHI}} '10},
  pages = {557--566},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1753326.1753409},
  urldate = {2022-12-03},
  abstract = {People strive to obtain self-knowledge. A class of systems called personal informatics is appearing that help people collect and reflect on personal information. However, there is no comprehensive list of problems that users experience using these systems, and no guidance for making these systems more effective. To address this, we conducted surveys and interviews with people who collect and reflect on personal information. We derived a stage-based model of personal informatics systems composed of five stages (preparation, collection, integration, reflection, and action) and identified barriers in each of the stages. These stages have four essential properties: barriers cascade to later stages; they are iterative; they are user-driven and/or system-driven; and they are uni-faceted or multi-faceted. From these properties, we recommend that personal informatics systems should 1) be designed in a holistic manner across the stages; 2) allow iteration between stages; 3) apply an appropriate balance of automated technology and user control within each stage to facilitate the user experience; and 4) explore support for associating multiple facets of people's lives to enrich the value of systems.},
  isbn = {978-1-60558-929-9},
  keywords = {barriers,collection,model,personal informatics,reflection},
  file = {C\:\\Users\\jseo1005\\Zotero\\storage\\9H2N6ELT\\Li et al. - 2010 - A stage-based model of personal informatics system.pdf}
}

@misc{initiativewaiMobileAccessibilityW3C,
  title = {Mobile {{Accessibility}} at {{W3C}}},
  author = {Initiative (WAI), W3C Web Accessibility},
  journal = {Web Accessibility Initiative (WAI)},
  urldate = {2022-12-14},
  abstract = {Accessibility resources free online from the international standards organization: W3C Web Accessibility Initiative (WAI).},
  howpublished = {https://www.w3.org/WAI/standards-guidelines/mobile/},
  langid = {english},
  file = {C\:\\Users\\jseo1005\\Zotero\\storage\\TC2Y4UAR\\mobile.html}
}

@inproceedings{jayantSupportingBlindPhotography2011,
  title = {Supporting Blind Photography},
  booktitle = {The Proceedings of the 13th International {{ACM SIGACCESS}} Conference on {{Computers}} and Accessibility},
  author = {Jayant, Chandrika and Ji, Hanjie and White, Samuel and Bigham, Jeffrey P.},
  year = {2011},
  month = oct,
  series = {{{ASSETS}} '11},
  pages = {203--210},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2049536.2049573},
  urldate = {2022-12-15},
  abstract = {Blind people want to take photographs for the same reasons as others -- to record important events, to share experiences, and as an outlet for artistic expression. Furthermore, both automatic computer vision technology and human-powered services can be used to give blind people feedback on their environment, but to work their best these systems need high-quality photos as input. In this paper, we present the results of a large survey that shows how blind people are currently using cameras. Next, we introduce EasySnap, an application that provides audio feedback to help blind people take pictures of objects and people and show that blind photographers take better photographs with this feedback. We then discuss how we iterated on the portrait functionality to create a new application called PortraitFramer designed specifically for this function. Finally, we present the results of an in-depth study with 15 blind and low-vision participants, showing that they could pick up how to successfully use the application very quickly.},
  isbn = {978-1-4503-0920-2},
  keywords = {blind,camera,photography,visually impaired},
  file = {C\:\\Users\\jseo1005\\Zotero\\storage\\5LG4ILNC\\Jayant et al. - 2011 - Supporting blind photography.pdf}
}

@article{jonesMobileHealthcarePeople2018,
  title = {Mobile {{Healthcare}} and {{People}} with {{Disabilities}}: {{Current State}} and {{Future Needs}}},
  shorttitle = {Mobile {{Healthcare}} and {{People}} with {{Disabilities}}},
  author = {Jones, Michael and Morris, John and Deruyter, Frank},
  year = {2018},
  month = mar,
  journal = {International Journal of Environmental Research and Public Health},
  volume = {15},
  number = {3},
  pages = {E515},
  issn = {1660-4601},
  doi = {10.3390/ijerph15030515},
  abstract = {Significant health disparities exist between the general population and people with disabilities, particularly with respect to chronic health conditions. Mobile healthcare-the delivery of healthcare via mobile communication devices-is witnessing tremendous growth and has been touted as an important new approach for management of chronic health conditions. At present, little is known about the current state of mobile healthcare for people with disabilities. Early evidence suggests they are not well represented in the growth of mobile healthcare, and particularly the proliferation of mobile health software applications (mHealth apps) for smartphones. Their omission in mHealth could lead to further health disparities. This article describes our research investigating the current state of mHealth apps targeting people with disabilities. Based on a multi-modal approach (literature review, Internet search, survey of disabled smartphone users), we confirm that people with disabilities are under-represented in the growth of mHealth. We identify several areas of future research and development needed to support the inclusion of people with disabilities in the mHealth revolution.},
  langid = {english},
  pmcid = {PMC5877060},
  pmid = {29538292},
  keywords = {Chronic Disease,chronic health conditions,Disabled Persons,Healthcare Disparities,Humans,information and communication technologies,Internet,mHealth,Mobile Applications,mobile healthcare,people with disabilities,Smartphone,software applications for smart phones,Telemedicine,United States},
  file = {C\:\\Users\\jseo1005\\Zotero\\storage\\QZSNKS8C\\Jones et al. - 2018 - Mobile Healthcare and People with Disabilities Cu.pdf}
}

@article{jonesMobileHealthcarePeople2018a,
  title = {Mobile Healthcare and People with Disabilities: {{Current}} State and Future Needs},
  author = {Jones, Michael and Morris, John and Deruyter, Frank},
  year = {2018},
  journal = {International Journal of Environmental Research and Public Health},
  volume = {15},
  number = {3},
  pages = {515},
  publisher = {{MDPI}},
  isbn = {1660-4601}
}

@article{jonesMobileHealthcarePeople2018b,
  title = {Mobile Healthcare and People with Disabilities: Current State and Future Needs},
  author = {Jones, Michael and Morris, John and Deruyter, Frank},
  year = {2018},
  journal = {International journal of environmental research and public health},
  volume = {15},
  number = {3},
  pages = {515},
  publisher = {{MDPI}},
  isbn = {1660-4601}
}

@article{jonesMobileHealthcarePeople2018c,
  title = {Mobile Healthcare and People with Disabilities: Current State and Future Needs},
  author = {Jones, Michael and Morris, John and Deruyter, Frank},
  year = {2018},
  journal = {International journal of environmental research and public health},
  volume = {15},
  number = {3},
  pages = {515},
  publisher = {{MDPI}},
  isbn = {1660-4601}
}

@article{jungFoundationsSystematicEvaluation2020,
  title = {Foundations for Systematic Evaluation and Benchmarking of a Mobile Food Logger in a Large-Scale Nutrition Study},
  author = {Jung, Jisu and {Wellard-Cole}, Lyndal and Cai, Colin and Koprinska, Irena and Yacef, Kalina and {Allman-Farinelli}, Margaret and Kay, Judy},
  year = {2020},
  journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
  volume = {4},
  number = {2},
  pages = {1--25},
  publisher = {{ACM New York, NY, USA}},
  isbn = {2474-9567}
}

@inproceedings{karkarTummyTrialsFeasibilityStudy2017,
  title = {{{TummyTrials}}: {{A}} Feasibility Study of Using Self-Experimentation to Detect Individualized Food Triggers},
  shorttitle = {{{TummyTrials}}},
  booktitle = {Proceedings of the 2017 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Karkar, Ravi and Schroeder, Jessica and Epstein, Daniel A. and Pina, Laura R. and Scofield, Jeffrey and Fogarty, James and Kientz, Julie A. and Munson, Sean A. and Vilardaga, Roger and Zia, Jasmine},
  year = {2017},
  month = may,
  series = {{{CHI}} '17},
  pages = {6850--6863},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3025453.3025480},
  urldate = {2022-12-13},
  abstract = {Diagnostic self-tracking, the recording of personal information to diagnose or manage a health condition, is a common practice, especially for people with chronic conditions. Unfortunately, many who attempt diagnostic self tracking have trouble accomplishing their goals. People often lack knowledge and skills needed to design and conduct scientifically rigorous experiments, and current tools provide little support. To address these shortcomings and explore opportunities for diagnostic self tracking, we designed, developed, and evaluated a mobile app that applies a self experimentation framework to support patients suffering from irritable bowel syndrome (IBS) in identifying their personal food triggers. TummyTrials aids a person in designing, executing, and analyzing self experiments to evaluate whether a specific food triggers their symptoms. We examined the feasibility of this approach in a field study with 15 IBS patients, finding that participants could use the tool to reliably undergo a self-experiment. However, we also discovered an underlying tension between scientific validity and the lived experience of self experimentation. We discuss challenges of applying clinical research methods in everyday life, motivating a need for the design of self experimentation systems to balance rigor with the uncertainties of everyday life.},
  isbn = {978-1-4503-4655-9},
  keywords = {food,irritable bowel syndrome,personal informatics,self-experimentation,self-tracking,symptom triggers},
  file = {C\:\\Users\\jseo1005\\Zotero\\storage\\W5BNEGDK\\Karkar et al. - 2017 - TummyTrials A Feasibility Study of Using Self-Exp.pdf}
}

@article{kimMobileHealthTechnology2019,
  title = {Mobile {{Health Technology Accessible}} to {{People}} with {{Visual Impairments}}},
  author = {Kim, Hyung Nam},
  year = {2019},
  pages = {14},
  abstract = {Although there is a potential of mobile health (mHealth) technology to facilitate self-care of people with visual impairments, less attention has been paid to accessibility of mHealth technology. The study aims at investigating the degree to which the mainstream mobile health applications (apps) that are commercially available on the market are compliant with accessibility standards. The Web Content Accessibility Guidelines (WCAG) 2.0 was used for checking the apps' accessibility. The accessibility problems found were associated with the WCAG accessibility principles \textendash{} i.e., perceivable, operable, understandable, and robust. It is recommended that user interfaces be presentable to users in ways that they can perceive; users receive a set of user interfaces that would not require any interaction that they cannot perform; users be able to understand the information and the operation of the user interfaces; and health apps provide a variety of features and functions that are compatible with the device and operating system version. As health care consumers with visual impairments are increasingly using health apps for self-care today, we should ensure that those health apps are adequately designed to accommodate those with visual impairments.},
  langid = {english},
  file = {C\:\\Users\\jseo1005\\Zotero\\storage\\8XF7KH2M\\Kim - 2019 - Mobile Health Technology Accessible to People with.pdf}
}

@inproceedings{kimMyMoveFacilitatingOlder2022,
  title = {{{MyMove}}: {{Facilitating}} Older Adults to Collect in-Situ Activity Labels on a Smartwatch with Speech},
  shorttitle = {{{MyMove}}},
  booktitle = {Proceedings of the 2022 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Kim, Young-Ho and Chou, Diana and Lee, Bongshin and Danilovich, Margaret and Lazar, Amanda and Conroy, David E. and Kacorri, Hernisa and Choe, Eun Kyoung},
  year = {2022},
  month = apr,
  series = {{{CHI}} '22},
  pages = {1--21},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3491102.3517457},
  urldate = {2022-12-13},
  abstract = {Current activity tracking technologies are largely trained on younger adults' data, which can lead to solutions that are not well-suited for older adults. To build activity trackers for older adults, it is crucial to collect training data with them. To this end, we examine the feasibility and challenges with older adults in collecting activity labels by leveraging speech. Specifically, we built MyMove, a speech-based smartwatch app to facilitate the in-situ labeling with a low capture burden. We conducted a 7-day deployment study, where 13 older adults collected their activity labels and smartwatch sensor data, while wearing a thigh-worn activity monitor. Participants were highly engaged, capturing 1,224 verbal reports in total. We extracted 1,885 activities with corresponding effort level and timespan, and examined the usefulness of these reports as activity labels. We discuss the implications of our approach and the collected dataset in supporting older adults through personalized activity tracking technologies.},
  isbn = {978-1-4503-9157-3},
  keywords = {activity labeling,experience sampling method,older adults,smartwatch,speech interaction},
  file = {C\:\\Users\\jseo1005\\Zotero\\storage\\8RNVYFIH\\Kim et al. - 2022 - MyMove Facilitating Older Adults to Collect In-Si.pdf}
}

@article{kimOmniTrackFlexibleSelftracking2017,
  title = {{{OmniTrack}}: {{A}} Flexible Self-Tracking Approach Leveraging Semi-Automated Tracking},
  shorttitle = {{{OmniTrack}}},
  author = {Kim, Young-Ho and Jeon, Jae Ho and Lee, Bongshin and Choe, Eun Kyoung and Seo, Jinwook},
  year = {2017},
  month = sep,
  journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
  volume = {1},
  number = {3},
  pages = {67:1--67:28},
  doi = {10.1145/3130930},
  urldate = {2022-12-13},
  abstract = {We now see an increasing number of self-tracking apps and wearable devices. Despite the vast number of available tools, however, it is still challenging for self-trackers to find apps that suit their unique tracking needs, preferences, and commitments. Furthermore, people are bounded by the tracking tools' initial design because it is difficult to modify, extend, or mash up existing tools. In this paper, we present OmniTrack, a mobile self-tracking system, which enables self-trackers to construct their own trackers and customize tracking items to meet their individual tracking needs. To inform the OmniTrack design, we first conducted semi-structured interviews (N = 12) and analyzed existing mobile tracking apps (N = 62). We then designed and developed OmniTrack as an Android mobile app, leveraging a semi-automated tracking approach that combines manual and automated tracking methods. We evaluated OmniTrack through a usability study (N = 10) and improved its interfaces based on the feedback. Finally, we conducted a 3-week deployment study (N = 21) to assess if people can capitalize on OmniTrack's flexible and customizable design to meet their tracking needs. From the study, we showed how participants used OmniTrack to create, revise, and appropriate trackers\textemdash ranging from a simple mood tracker to a sophisticated daily activity tracker. We discuss how OmniTrack positively influences and supports self-trackers' tracking practices over time, and how to further improve OmniTrack by providing more appropriate visualizations and sharable templates, incorporating external contexts, and supporting researchers' unique data collection needs.},
  keywords = {customization.,health,mobile apps,personal informatics,self-monitoring,Self-tracking,semi-automated tracking,tracking apps,wellness},
  file = {C\:\\Users\\jseo1005\\Zotero\\storage\\C3INK9NB\\Kim et al. - 2017 - OmniTrack A Flexible Self-Tracking Approach Lever.pdf}
}

@article{kostyraFoodShoppingSensory2017,
  title = {Food Shopping, Sensory Determinants of Food Choice and Meal Preparation by Visually Impaired People. {{Obstacles}} and Expectations in Daily Food Experiences},
  author = {Kostyra, Eliza and {\.Z}akowska-Biemans, Sylwia and {\'S}niegocka, Katarzyna and Piotrowska, Anna},
  year = {2017},
  journal = {Appetite},
  volume = {113},
  pages = {14--22},
  publisher = {{Elsevier}},
  isbn = {0195-6663}
}

@article{kulkarniSpeechLanguagePractitioners2022,
  title = {Speech and {{Language Practitioners}}' {{Experiences}} of {{Commercially Available Voice-Assisted Technology}}: {{Web-Based Survey Study}}},
  shorttitle = {Speech and {{Language Practitioners}}' {{Experiences}} of {{Commercially Available Voice-Assisted Technology}}},
  author = {Kulkarni, Pranav and Duffy, Orla and Synnott, Jonathan and Kernohan, W. George and McNaney, Roisin},
  year = {2022},
  month = jan,
  journal = {JMIR Rehabilitation and Assistive Technologies},
  volume = {9},
  number = {1},
  pages = {e29249},
  publisher = {{JMIR Publications Inc., Toronto, Canada}},
  doi = {10.2196/29249},
  urldate = {2022-03-28},
  abstract = {Background: Speech and language therapy involves the identification, assessment, and treatment of children and adults who have difficulties with communication, eating, drinking, and swallowing. Globally, pressing needs outstrip the availability of qualified practitioners who, of necessity, focus on individuals with advanced needs. The potential of voice-assisted technology (VAT) to assist people with speech impairments is an emerging area of research but empirical work exploring its professional adoption is limited. Objective: This study aims to explore the professional experiences of speech and language therapists (SaLTs) using VAT with their clients to identify the potential applications and barriers to VAT adoption and thereby inform future directions of research. Methods: A 23-question survey was distributed to the SaLTs from the United Kingdom using a web-based platform, eliciting both checkbox and free-text responses, to questions on perceptions and any use experiences of VAT. Data were analyzed descriptively with content analysis of free text, providing context to their specific experiences of using VAT in practice, including barriers and opportunities for future use. Results: A total of 230 UK-based professionals fully completed the survey; most were technologically competent and were aware of commercial VATs (such as Alexa and Google Assistant). However, only 49 (21.3\%) SaLTs had used VAT with their clients and described 57 use cases. They reported using VAT with 10 different client groups, such as people with dysarthria and users of augmentative and alternative communication technologies. Of these, almost half (28/57, 49\%) used the technology to assist their clients with day-to-day tasks, such as web browsing, setting up reminders, sending messages, and playing music. Many respondents (21/57, 37\%) also reported using the technology to improve client speech, to facilitate speech practice at home, and to enhance articulation and volume. Most reported a positive impact of VAT use, stating improved independence (22/57, 39\%), accessibility (6/57, 10\%), and confidence (5/57, 8\%). Some respondents reported increased client communication (5/57, 9\%) and sociability (3/57, 5\%). Reasons given for not using VAT in practice included lack of opportunity (131/181, 72.4\%) and training (63/181, 34.8\%). Most respondents (154/181, 85.1\%) indicated that they would like to try VAT in the future, stating that it could have a positive impact on their clients' speech, independence, and confidence. Conclusions: VAT is used by some UK-based SaLTs to enable communication tasks at home with their clients. However, its wider adoption may be limited by a lack of professional opportunity. Looking forward, additional benefits are promised, as the data show a level of engagement, empowerment, and the possibility of achieving therapeutic outcomes in communication impairment. The disparate responses suggest that this area is ripe for the development of evidence-based clinical practice, starting with a clear definition, outcome measurement, and professional standardization.},
  copyright = {Unless stated otherwise, all articles are open-access distributed under the terms of the Creative Commons Attribution License (http://creativecommons.org/licenses/by/2.0/), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work ("first published in the Journal of Medical Internet Research...") is properly cited with original URL and bibliographic citation information. The complete bibliographic information, a link to the original publication on http://www.jmir.org/, as well as this copyright and license information must be included.},
  langid = {english},
  file = {C\:\\Users\\jseo1005\\Zotero\\storage\\77IWAJFI\\Kulkarni et al. - 2022 - Speech and Language Practitioners’ Experiences of .pdf;C\:\\Users\\jseo1005\\Zotero\\storage\\8QHV9JET\\e29249.html}
}

@article{ladnerDesignUserEmpowerment2015,
  title = {Design for User Empowerment},
  author = {Ladner, Richard E.},
  year = {2015},
  journal = {Interactions},
  volume = {22},
  number = {2},
  pages = {24--29},
  publisher = {{ACM New York, NY, USA}},
  isbn = {1072-5520}
}

@book{leeMobileDataVisualization2021,
  title = {Mobile Data Visualization},
  author = {Lee, Bongshin and Dachselt, Raimund and Isenberg, Petra and Choe, Eun Kyoung},
  year = {2021},
  month = dec,
  publisher = {{CRC Press}},
  abstract = {Mobile Data Visualization is about facilitating access to and understanding of data on mobile devices. Wearable trackers, mobile phones, and tablets are used by millions of people each day to read weather maps, financial charts, or personal health meters. What is required to create effective visualizations for mobile devices? This book introduces key concepts of mobile data visualization and discusses opportunities and challenges from both research and practical perspectives. Mobile Data Visualization is the first book to provide an overview of how to effectively visualize, analyze, and communicate data on mobile devices. Drawing from the expertise, research, and experience of an international range of academics and practitioners from across the domains of Visualization, Human Computer Interaction, and Ubiquitous Computing, the book explores the challenges of mobile visualization and explains how it differs from traditional data visualization. It highlights opportunities for reaching new audiences with engaging, interactive, and compelling mobile content. In nine chapters, this book presents interesting perspectives on mobile data visualization including: how to characterize and classify mobile visualizations; how to interact with them while on the go and with limited attention spans; how to adapt them to various mobile contexts; specific methods on how to design and evaluate them; reflections on privacy, ethical and other challenges, as well as an outlook to a future of ubiquitous visualization. This accessible book is a valuable and rich resource for visualization designers, practitioners, researchers, and students alike.},
  googlebooks = {e\_xNEAAAQBAJ},
  isbn = {978-1-00-052280-8},
  langid = {english},
  keywords = {Computers / Data Science / Data Modeling \& Design,Computers / Data Science / Data Visualization,Computers / Programming / Mobile Devices}
}

@misc{leePersonalHealthData,
  title = {Personal Health Data Tracking by Blind and Low-Vision People: {{A}} Survey Study},
  shorttitle = {Personal {{Health Data Tracking}} by {{Blind}} and {{Low-Vision People}}},
  author = {Lee, Jarrett G.W. and Lee, Kyungyeon and Lee, Bongshin and Choi, Soyoung and Seo, JooYoung and Choe, Eun Kyoung},
  urldate = {2022-12-12},
  abstract = {Journal of Medical Internet Research - International Scientific Journal for Medical Research, Information and Communication on the Internet},
  langid = {english},
  file = {C\:\\Users\\jseo1005\\Zotero\\storage\\NCN6LCIL\\43917.html}
}

@article{leeReachingBroaderAudiences2020,
  title = {Reaching Broader Audiences with Data Visualization},
  author = {Lee, Bongshin and Choe, Eun Kyoung and Isenberg, Petra and Marriott, Kim and Stasko, John},
  year = {2020},
  month = mar,
  journal = {IEEE Computer Graphics and Applications},
  volume = {40},
  number = {2},
  pages = {82--90},
  issn = {1558-1756},
  doi = {10.1109/MCG.2020.2968244},
  abstract = {The visualization research community can and should reach broader audiences beyond data-savvy groups of people, because these audiences could also greatly benefit from visual access to data. In this article, we discuss four research topics\textemdash personal data visualization, data visualization on mobile devices, inclusive data visualization, and multimodal interaction for data visualization\textemdash that, individually and collaboratively, would help us reach broader audiences with data visualization, making data more accessible.},
  keywords = {Data visualization,Public infrastructure,Smart phones,Software tools,Visualization},
  file = {C\:\\Users\\jseo1005\\Zotero\\storage\\MZBGMVKP\\Lee et al. - 2020 - Reaching Broader Audiences With Data Visualization.pdf;C\:\\Users\\jseo1005\\Zotero\\storage\\G6F64KKW\\9023497.html}
}

@inproceedings{leeRevisitingBlindPhotography2019,
  title = {Revisiting {{Blind Photography}} in the {{Context}} of {{Teachable Object Recognizers}}},
  booktitle = {Proceedings of the 21st {{International ACM SIGACCESS Conference}} on {{Computers}} and {{Accessibility}}},
  author = {Lee, Kyungjun and Hong, Jonggi and Pimento, Simone and Jarjue, Ebrima and Kacorri, Hernisa},
  year = {2019},
  month = oct,
  series = {{{ASSETS}} '19},
  pages = {83--95},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3308561.3353799},
  urldate = {2022-12-15},
  abstract = {For people with visual impairments, photography is essential in identifying objects through remote sighted help and image recognition apps. This is especially the case for teachable object recognizers, where recognition models are trained on user's photos. Here, we propose real-time feedback for communicating the location of an object of interest in the camera frame. Our audio-haptic feedback is powered by a deep learning model that estimates the object center location based on its proximity to the user's hand. To evaluate our approach, we conducted a user study in the lab, where participants with visual impairments (N=9) used our feedback to train and test their object recognizer in vanilla and cluttered environments. We found that very few photos did not include the object (2\% in the vanilla and 8\% in the cluttered) and the recognition performance was promising even for participants with no prior camera experience. Participants tended to trust the feedback even though they know it can be wrong. Our cluster analysis indicates that better feedback is associated with photos that include the entire object. Our results provide insights into factors that can degrade feedback and recognition performance in teachable interfaces.},
  isbn = {978-1-4503-6676-2},
  keywords = {hand,object recognition,sonification,visual impairments},
  file = {C\:\\Users\\jseo1005\\Zotero\\storage\\HMK3TIT7\\Lee et al. - 2019 - Revisiting Blind Photography in the Context of Tea.pdf}
}

@inproceedings{limTradeoffAutomationAccuracy2017,
  title = {Trade-off between Automation and Accuracy in Mobile Photo Recognition Food Logging},
  booktitle = {Proceedings of the {{Fifth International Symposium}} of {{Chinese CHI}}},
  author = {Lim, Brian Y. and Chng, Xinni and Zhao, Shengdong},
  year = {2017},
  pages = {53--59}
}

@incollection{lippincotSurveyUserNeeds2020,
  title = {Survey of {{User Needs}}: {{Mobile Apps}} for {{mHealth}} and {{People}} with {{Disabilities}}},
  shorttitle = {Survey of {{User Needs}}},
  booktitle = {Computers {{Helping People}} with {{Special Needs}}},
  author = {Lippincot, Ben and Thompson, Nicole and Morris, John and Jones, Mike and DeRuyter, Frank},
  editor = {Miesenberger, Klaus and Manduchi, Roberto and Covarrubias Rodriguez, Mario and Pe{\v n}{\'a}z, Petr},
  year = {2020},
  volume = {12377},
  pages = {266--273},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-58805-2\_32},
  urldate = {2022-05-04},
  abstract = {This paper presents data and analysis from survey research conducted by the Rehabilitation Engineering Research Center on Information and Communication Technology Access for Mobile Rehabilitation (mRehab RERC) on the use and unmet needs for mHealth mobile apps by people with disabilities in the United States. Quantitative and qualitative data are reported on user experiences with mHealth apps to map the behavior, interests and needs of people with specific types of disability (physical, cognitive, sensory, emotional/psychological, and speech). Summary results are presented for all respondents and each disability type. Slightly more than half of the participants in this sample (53.2\%) reported using mHealth apps. Fitness and exercise apps were the mHealth apps most used by respondents with disabilities, followed by hospital/clinical portal apps. Symptom and disease management apps are the least commonly used, even though these would seem to be important for people with chronic conditions. Text-based responses regarding unmet needs for mHealth apps can be sorted into accessibility needs and functionality needs. In general, respondents with sensory limitations were more likely to identify accessibility needs. However, all disability groups identified both types of unmet needs. These results can help inform research and development efforts to provide mHealth apps that meet the needs of people with disabilities.},
  isbn = {978-3-030-58804-5 978-3-030-58805-2},
  langid = {english},
  file = {C\:\\Users\\jseo1005\\Zotero\\storage\\R7LJUIZA\\Lippincot et al. - 2020 - Survey of User Needs Mobile Apps for mHealth and .pdf}
}

@misc{LoseIt,
  title = {Lose {{It}}!},
  urldate = {2022-12-12},
  abstract = {With 40 million downloads and over 100 million pounds lost, Lose It! is on a mission to help the world achieve a healthy weight through calorie tracking and personal nutrition education.},
  howpublished = {https://www.loseit.com/},
  langid = {english},
  file = {C\:\\Users\\jseo1005\\Zotero\\storage\\PCPXWBMW\\www.loseit.com.html}
}

@inproceedings{luoCodesigningFoodTrackers2019,
  title = {Co-Designing Food Trackers with Dietitians: {{Identifying}} Design Opportunities for Food Tracker Customization},
  shorttitle = {Co-{{Designing Food Trackers}} with {{Dietitians}}},
  booktitle = {Proceedings of the 2019 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Luo, Yuhan and Liu, Peiyi and Choe, Eun Kyoung},
  year = {2019},
  month = may,
  series = {{{CHI}} '19},
  pages = {1--13},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3290605.3300822},
  urldate = {2022-12-13},
  abstract = {We report co-design workshops with registered dietitians conducted to identify opportunities for designing customizable food trackers. Dietitians typically see patients who have different dietary problems, thus having different information needs. However, existing food trackers such as paper-based diaries and mobile apps are rarely customizable, making it difficult to capture necessary data for both patients and dietitians. During the co-design sessions, dietitians created representative patient personas and designed food trackers for each persona. We found a wide range of potential tracking items such as food, reflection, symptom, activity, and physical state. Depending on patients' dietary problems and dietitians' practice, the necessity and importance of these tracking items vary. We identify opportunities for patients and healthcare providers to collaborate around data tracking and sharing through customization. We also discuss how to structure co-design workshops to solicit the design considerations of self-tracking tools for patients with specific health problems.},
  isbn = {978-1-4503-5970-2},
  keywords = {co-design workshop,customization,food tracking,health,personal informatics,self-tracking},
  file = {C\:\\Users\\jseo1005\\Zotero\\storage\\XVB2QH7K\\Luo et al. - 2019 - Co-Designing Food Trackers with Dietitians Identi.pdf}
}

@inproceedings{luoFoodScrapPromotingRich2021,
  title = {{{FoodScrap}}: {{Promoting}} Rich Data Capture and Reflective Food Journaling through Speech Input},
  shorttitle = {{{FoodScrap}}},
  booktitle = {Designing {{Interactive Systems Conference}} 2021},
  author = {Luo, Yuhan and Kim, Young-Ho and Lee, Bongshin and Hassan, Naeemul and Choe, Eun Kyoung},
  year = {2021},
  month = jun,
  series = {{{DIS}} '21},
  pages = {606--618},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3461778.3462074},
  urldate = {2022-12-03},
  abstract = {The factors influencing people's food decisions, such as one's mood and eating environment, are important information to foster self-reflection and to develop personalized healthy diet. But, it is difficult to consistently collect them due to the heavy data capture burden. In this work, we examine how speech input supports capturing everyday food practice through a week-long data collection study (N = 11). We deployed FoodScrap, a speech-based food journaling app that allows people to capture food components, preparation methods, and food decisions. Using speech input, participants detailed their meal ingredients and elaborated their food decisions by describing the eating moments, explaining their eating strategy, and assessing their food practice. Participants recognized that speech input facilitated self-reflection, but expressed concerns around re-recording, mental load, social constraints, and privacy. We discuss how speech input can support low-burden and reflective food journaling and opportunities for effectively processing and presenting large amounts of speech data.},
  isbn = {978-1-4503-8476-6},
  keywords = {Food tracking,personal informatics,self-tracking,speech input,speech interface design},
  file = {C\:\\Users\\jseo1005\\Zotero\\storage\\SSSFHWTT\\Luo et al. - 2021 - FoodScrap Promoting Rich Data Capture and Reflect.pdf}
}

@article{luoInfluenceCOVID19Lockdowns2021,
  title = {Influence of {{COVID-19 Lockdowns}} on the {{Usage}} of a {{Vision Assistance App Among Global Users With Visual Impairment}}: {{Big Data Analytics Study}}},
  shorttitle = {Influence of {{COVID-19 Lockdowns}} on the {{Usage}} of a {{Vision Assistance App Among Global Users With Visual Impairment}}},
  author = {Luo, Gang and Pundlik, Shrinivas},
  year = {2021},
  month = may,
  journal = {Journal of Medical Internet Research},
  volume = {23},
  number = {5},
  pages = {e26283},
  publisher = {{JMIR Publications Inc., Toronto, Canada}},
  doi = {10.2196/26283},
  urldate = {2022-03-28},
  abstract = {Background: Millions of individuals with visual impairment use vision assistance apps to help with their daily activities. The most widely used vision assistance apps are magnifier apps. It is still largely unknown what the apps are used for. Lack of insight into the visual needs of individuals with visual impairment is a hurdle for the development of more effective assistive technologies. Objective: This study aimed to investigate how needs for visual aids may vary with social activities, by observing the changes in the usage of a smartphone magnifier app when many users take breaks from work. Methods: The number of launches of the SuperVision Magnifier app was determined retrospectively from 2018 to 2020 from among active users worldwide. The fluctuation in app usage was examined by comparing weekday vs weekend periods, Christmas and new year vs nonholiday seasons, and COVID-19 lockdowns vs the easing of restriction during the pandemic. Results: On average, the app was used 262,466 times by 38,237 users each month in 2020 worldwide. There were two major trough points on the timeline of weekly app usage, one aligned with the COVID-19 lockdowns in April 2020 and another aligned with the Christmas and new year week in 2018 and 2019. The app launches declined by 6947 (11\% decline; P\&lt;.001) during the lockdown and by 5212 (9\% decline; P=.001) during the holiday weeks. There was no significant decline during March to May 2019. App usage compensated for seasonal changes was 8.6\% less during weekends than during weekdays (P\&lt;.001). Conclusions: The need for vision assistance technology was slightly lower during breaks and lockdowns, probably because the activities at home were different and less visually demanding. Nevertheless, for the entire user population, the needs for visual aids are still substantial.},
  langid = {english},
  file = {C\:\\Users\\jseo1005\\Zotero\\storage\\PZZN9J5D\\Luo and Pundlik - 2021 - Influence of COVID-19 Lockdowns on the Usage of a .pdf;C\:\\Users\\jseo1005\\Zotero\\storage\\RILGBRE9\\e26283.html}
}

@article{luoInterrelationshipsPatientsData2020,
  title = {Interrelationships between Patients' Data Tracking Practices, Data Sharing Practices, and Health Literacy: {{Onsite}} Survey Study},
  shorttitle = {Interrelationships {{Between Patients}}' {{Data Tracking Practices}}, {{Data Sharing Practices}}, and {{Health Literacy}}},
  author = {Luo, Yuhan and Oh, Chi Young and Jean, Beth St and Choe, Eun Kyoung},
  year = {2020},
  month = dec,
  journal = {Journal of Medical Internet Research},
  volume = {22},
  number = {12},
  pages = {e18937},
  publisher = {{JMIR Publications Inc., Toronto, Canada}},
  doi = {10.2196/18937},
  urldate = {2022-12-08},
  abstract = {Background: Although the use of patient-generated data (PGD) in the optimization of patient care shows great promise, little is known about whether patients who track their PGD necessarily share the data with their clinicians. Meanwhile, health literacy\textemdash an important construct that captures an individual's ability to manage their health and to engage with their health care providers\textemdash has often been neglected in prior studies focused on PGD tracking and sharing. To leverage the full potential of PGD, it is necessary to bridge the gap between patients' data tracking and data sharing practices by first understanding the interrelationships between these practices and the factors contributing to these practices. Objective: This study aims to systematically examine the interrelationships between PGD tracking practices, data sharing practices, and health literacy among individual patients. Methods: We surveyed 109 patients at the time they met with a clinician at a university health center, unlike prior research that often examined patients' retrospective experience after some time had passed since their clinic visit. The survey consisted of 39 questions asking patients about their PGD tracking and sharing practices based on their current clinical encounter. The survey also contained questions related to the participants' health literacy. All the participants completed the survey on a tablet device. The onsite survey study enabled us to collect ecologically valid data based on patients' immediate experiences situated within their clinic visit. Results: We found no evidence that tracking PGD was related to self-reports of having sufficient information to manage one's health; however, the number of data types participants tracked positively related to their self-assessed ability to actively engage with health care providers. Participants' data tracking practices and their health literacy did not relate to their data sharing practices; however, their ability to engage with health care providers positively related to their willingness to share their data with clinicians in the future. Participants reported several benefits of, and barriers to, sharing their PGD with clinicians. Conclusions: Although tracking PGD could help patients better engage with health care providers, it may not provide patients with sufficient information to manage their health. The gaps between tracking and sharing PGD with health care providers call for efforts to inform patients of how their data relate to their health and to facilitate efficient clinician-patient communication. To realize the full potential of PGD and to promote individuals' health literacy, empowering patients to effectively track and share their PGD is important\textemdash both technologies and health care providers can play important roles.},
  langid = {english},
  file = {C\:\\Users\\jseo1005\\Zotero\\storage\\W8EMJZD4\\Luo et al. - 2020 - Interrelationships Between Patients’ Data Tracking.pdf;C\:\\Users\\jseo1005\\Zotero\\storage\\75ANGRA5\\e18937.html}
}

@article{madrigal-cadavidDesignDevelopmentMobile2020,
  title = {Design and Development of a Mobile App of Drug Information for People with Visual Impairment},
  author = {{Madrigal-Cadavid}, Juliana and Amariles, Pedro and {Pino-Mar{\'i}n}, Daniel and Granados, Johan and Giraldo, Newar},
  year = {2020},
  month = jan,
  journal = {Research in Social and Administrative Pharmacy},
  volume = {16},
  number = {1},
  pages = {62--67},
  issn = {1551-7411},
  doi = {10.1016/j.sapharm.2019.02.013},
  urldate = {2022-12-12},
  abstract = {Background People with visual impairment presents difficulties to access the labels information of medicines. In this sense, technological tools can contribute to improve access to this information and the appropriate use of medicines in this population. However, currently, in Colombia, there are no tools to facilitate this process. Objective To design and development of a mobile app of drug information for people with visual impairment, which allows them to access information for the appropriate use of medicines. Methods A user-centered design process is carried out in four phases was used: a) Identification the needs and barriers for appropriate use of medicines; b) Lifting of requirements, c) Interface design and prototyping, and development of the mobile app, and d) Usability test. Results The study involved 48 people with visual disability, of which 69\% required assistance for the use of medicines. The main barriers identified were access to information and dosing. A total of ten user requirements were identified, based on these and international accessibility standards FarmaceuticApp was designed and developed, incorporating the problems that were identified in the usability test. Conclusion A mobile app of drug information for people with visual impairment using a user-centered design process was designed and developed, highlighting the importance of involving the users and other stakeholders in the design and development m-health technologies. FarmaceuticApp could contribute to the appropriate use of medicines and improve therapeutic adherence, as well as autonomy and independence in people with visual impairment.},
  langid = {english},
  keywords = {Mobile applications,Mobile health,Rational use of medicines,Technological tools,Visual disability},
  file = {C\:\\Users\\jseo1005\\Zotero\\storage\\4KRJ2SUG\\S1551741119301317.html}
}

@article{marriottInclusiveDataVisualization2021,
  title = {Inclusive Data Visualization for People with Disabilities: {{A}} Call to Action},
  shorttitle = {Inclusive Data Visualization for People with Disabilities},
  author = {Marriott, Kim and Lee, Bongshin and Butler, Matthew and Cutrell, Ed and Ellis, Kirsten and Goncu, Cagatay and Hearst, Marti and McCoy, Kathleen and Szafir, Danielle Albers},
  year = {2021},
  month = apr,
  journal = {Interactions},
  volume = {28},
  number = {3},
  pages = {47--51},
  issn = {1072-5520},
  doi = {10.1145/3457875},
  urldate = {2022-12-14},
  file = {C\:\\Users\\jseo1005\\Zotero\\storage\\FUSSS6WY\\Marriott et al. - 2021 - Inclusive data visualization for people with disab.pdf}
}

@article{masinaInvestigatingAccessibilityVoice2020,
  title = {Investigating the {{Accessibility}} of {{Voice Assistants With Impaired Users}}: {{Mixed Methods Study}}},
  shorttitle = {Investigating the {{Accessibility}} of {{Voice Assistants With Impaired Users}}},
  author = {Masina, Fabio and Orso, Valeria and Pluchino, Patrik and Dainese, Giulia and Volpato, Stefania and Nelini, Cristian and Mapelli, Daniela and Spagnolli, Anna and Gamberini, Luciano},
  year = {2020},
  month = sep,
  journal = {Journal of Medical Internet Research},
  volume = {22},
  number = {9},
  pages = {e18431},
  publisher = {{JMIR Publications Inc., Toronto, Canada}},
  doi = {10.2196/18431},
  urldate = {2022-03-28},
  abstract = {Background: Voice assistants allow users to control appliances and functions of a smart home by simply uttering a few words. Such systems hold the potential to significantly help users with motor and cognitive disabilities who currently depend on their caregiver even for basic needs (eg, opening a door). The research on voice assistants is mainly dedicated to able-bodied users, and studies evaluating the accessibility of such systems are still sparse and fail to account for the participants' actual motor, linguistic, and cognitive abilities. Objective: The aim of this work is to investigate whether cognitive and/or linguistic functions could predict user performance in operating an off-the-shelf voice assistant (Google Home). Methods: A group of users with disabilities (n=16) was invited to a living laboratory and asked to interact with the system. Besides collecting data on their performance and experience with the system, their cognitive and linguistic skills were assessed using standardized inventories. The identification of predictors (cognitive and/or linguistic) capable of accounting for an efficient interaction with the voice assistant was investigated by performing multiple linear regression models. The best model was identified by adopting a selection strategy based on the Akaike information criterion (AIC). Results: For users with disabilities, the effectiveness of interacting with a voice assistant is predicted by the Mini-Mental State Examination (MMSE) and the Robertson Dysarthria Profile (specifically, the ability to repeat sentences), as the best model shows (AIC=130.11). Conclusions: Users with motor, linguistic, and cognitive impairments can effectively interact with voice assistants, given specific levels of residual cognitive and linguistic skills. More specifically, our paper advances practical indicators to predict the level of accessibility of speech-based interactive systems. Finally, accessibility design guidelines are introduced based on the performance results observed in users with disabilities.},
  langid = {english},
  file = {C\:\\Users\\jseo1005\\Zotero\\storage\\43Z9ZNHN\\Masina et al. - 2020 - Investigating the Accessibility of Voice Assistant.pdf;C\:\\Users\\jseo1005\\Zotero\\storage\\PQ6CJQ7H\\e18431.html}
}

@inproceedings{mateusAccessibilityMobileApplications2020,
  title = {Accessibility of Mobile Applications: Evaluation by Users with Visual Impairment and by Automated Tools},
  shorttitle = {Accessibility of Mobile Applications},
  booktitle = {Proceedings of the 19th {{Brazilian Symposium}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Mateus, Delvani Ant{\^o}nio and Silva, Carlos Alberto and Eler, Marcelo Medeiros and Freire, Andr{\'e} Pimenta},
  year = {2020},
  month = oct,
  pages = {1--10},
  publisher = {{ACM}},
  address = {{Diamantina Brazil}},
  doi = {10.1145/3424953.3426633},
  urldate = {2022-02-20},
  isbn = {978-1-4503-8172-7},
  langid = {english},
  file = {C\:\\Users\\jseo1005\\Zotero\\storage\\8FJAVQ29\\Mateus et al. - 2020 - Accessibility of mobile applications evaluation b.pdf}
}

@book{mattelmakiDesignProbes2006,
  title = {Design Probes},
  author = {Mattelm{\"a}ki, Tuuli},
  year = {2006},
  publisher = {{Aalto University}},
  issn = {0782-1832 (printed)},
  urldate = {2022-12-07},
  abstract = {The study focuses on an innovative user centred design approach called probes. Probes are based on self-documenting, they are explorative and design oriented, and they aim at revealing users' personal perspectives to enrich design and support empathy. They have been applied both to experimental and business projects but a detailed consideration has not yet appeared. This book describes the probes, discusses the reasons and practices of applying them.},
  isbn = {978-951-558-212-6},
  langid = {english},
  annotation = {Accepted: 2013-12-10T10:01:08Z},
  file = {C\:\\Users\\jseo1005\\Zotero\\storage\\N654PRFH\\Mattelmäki - 2006 - Design probes.pdf;C\:\\Users\\jseo1005\\Zotero\\storage\\SSS59CC4\\11829.html}
}

@article{meyerAccessibleWebsitesMobile2020,
  title = {Accessible Websites and Mobile Applications under the {{ADA}}: {{The}} Lack of Legal Guidelines and What This Means for Businesses and Their Customers},
  author = {Meyer, Josephine},
  year = {2020}
}

@misc{microsoftDiversityInclusionReport,
  title = {Diversity \& {{Inclusion Report}}},
  author = {Microsoft},
  journal = {Diversity \& Inclusion Report},
  urldate = {2022-12-14},
  abstract = {Every day, we strive to strengthen a culture of inclusion at Microsoft and drive positive change in our company and beyond.},
  howpublished = {https://www.microsoft.com/en-us/diversity/inside-microsoft/annual-report},
  langid = {american},
  file = {C\:\\Users\\jseo1005\\Zotero\\storage\\HVN954UL\\annual-report.html}
}

@misc{microsoftSeeingAI,
  title = {Seeing {{AI}}},
  author = {Microsoft},
  urldate = {2022-12-08},
  abstract = {Seeing AI app helps people with vision impairment convert visual info into audio. Download for free in English, Dutch, German, French, Japanese, and Spanish.},
  howpublished = {https://www.microsoft.com/en-us/ai/seeing-ai},
  langid = {american},
  file = {C\:\\Users\\jseo1005\\Zotero\\storage\\L79UE8JL\\seeing-ai.html}
}

@article{milneAccessibilityMobileHealth2014,
  title = {The Accessibility of Mobile Health Sensors for Blind Users},
  author = {Milne, Lauren R. and Bennett, Cynthia L. and Ladner, Richard E.},
  year = {2014},
  month = dec,
  publisher = {{California State University, Northridge.}},
  issn = {2330-4219},
  urldate = {2022-12-12},
  abstract = {Mobile health (mHealth) applications are becoming popular and could be useful for people who are blind as they allow the data from mainstream health sensors to be accessed on the smartphone. However, in order for the health sensors to be accessible, the smartphone applications must be accessible. In early 2014, we conducted a survey of the accessibility of nine mHealth applications for the iPhone and found that none of them met our criteria (based on the guidelines provided by Apple and Section 508) for being accessible. We found that the majority of the accessibility problems encountered were relatively simple and believe that it would only take a small amount of effort on the part of developers to fulfill the potential for mHealth applications to make mainstream health sensors fully accessible to blind users of smartphones.},
  langid = {american},
  annotation = {Accepted: 2015-01-15T23:12:39Z},
  file = {C\:\\Users\\jseo1005\\Zotero\\storage\\4RYIU8AT\\Milne et al. - 2014 - The Accessibility of Mobile Health Sensors for Bli.pdf}
}

@misc{MyFitnessPal,
  title = {{{MyFitnessPal}}},
  urldate = {2022-12-12},
  abstract = {Take control of your goals. Track calories, break down ingredients, and log activities with MyFitnessPal.},
  howpublished = {https://www.myfitnesspal.com},
  langid = {english},
  file = {C\:\\Users\\jseo1005\\Zotero\\storage\\HNL3T62A\\www.myfitnesspal.com.html}
}

@misc{MyFoodDiary,
  title = {{{MyFoodDiary}}\textregistered},
  urldate = {2022-12-12},
  abstract = {Eat better, feel better. Track 21 nutrients with our food diary. Count calories, net carbs, vitamins, protein, \& more. Get started today!},
  howpublished = {https://www.myfooddiary.com/},
  langid = {english},
  file = {C\:\\Users\\jseo1005\\Zotero\\storage\\FXJSK25B\\www.myfooddiary.com.html}
}

@misc{MyNetDiary,
  title = {{{MyNetDiary}}},
  journal = {MyNetDiary},
  urldate = {2022-12-12},
  abstract = {Free calorie counter and nutrition assistant app. MyNetDiary is your digital diet assistant for weight loss. Our calorie counter is easy to use, and the application goes way beyond that. Try MyNetDiary today!},
  howpublished = {https://www.mynetdiary.com/},
  langid = {english},
  file = {C\:\\Users\\jseo1005\\Zotero\\storage\\Q7YGTKA5\\www.mynetdiary.com.html}
}

@misc{nationalfederationoftheblindDefinitionBlindness,
  title = {The {{Definition}} of {{Blindness}}},
  author = {{National Federation of the Blind}},
  urldate = {2022-12-02},
  howpublished = {https://nfb.org/sites/default/files/images/nfb/publications/books/books1/kj07.htm},
  file = {C\:\\Users\\jseo1005\\Zotero\\storage\\L7PNHT7K\\kj07.html}
}

@article{nicholsonShopTalkIndependentBlind2009,
  title = {{{ShopTalk}}: {{Independent Blind Shopping Through Verbal Route Directions}} and {{Barcode Scans}}},
  shorttitle = {{{ShopTalk}}},
  author = {Nicholson, John and Kulyukin, Vladimir and Coster, Daniel},
  year = {2009},
  month = mar,
  journal = {The Open Rehabilitation Journal},
  volume = {2},
  number = {1},
  pages = {11--23},
  issn = {18749437},
  doi = {10.2174/1874943700902010011},
  urldate = {2022-12-15},
  langid = {english},
  file = {C\:\\Users\\jseo1005\\Zotero\\storage\\LHQVRS3M\\Nicholson et al. - 2009 - ShopTalk Independent Blind Shopping Through Verba.pdf}
}

@inproceedings{nielsenHeuristicEvaluationUser1990,
  title = {Heuristic Evaluation of User Interfaces},
  booktitle = {Proceedings of the {{SIGCHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Nielsen, Jakob and Molich, Rolf},
  year = {1990},
  month = mar,
  series = {{{CHI}} '90},
  pages = {249--256},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/97243.97281},
  urldate = {2022-12-07},
  abstract = {Heuristic evaluation is an informal method of usability analysis where a number of evaluators are presented with an interface design and asked to comment on it. Four experiments showed that individual evaluators were mostly quite bad at doing such heuristic evaluations and that they only found between 20 and 51\% of the usability problems in the interfaces they evaluated. On the other hand, we could aggregate the evaluations from several evaluators to a single evaluation and such aggregates do rather well, even when they consist of only three to five people.},
  isbn = {978-0-201-50932-8},
  file = {C\:\\Users\\jseo1005\\Zotero\\storage\\HFFI8LRY\\Nielsen and Molich - 1990 - Heuristic evaluation of user interfaces.pdf}
}

@misc{Noom,
  title = {Noom},
  shorttitle = {Noom},
  journal = {Noom, Inc.},
  urldate = {2022-12-12},
  abstract = {Noom combines the power of technology with the empathy of real human coaches to deliver successful behavior change at scale.},
  howpublished = {https://www.noom.com},
  langid = {english},
  file = {C\:\\Users\\jseo1005\\Zotero\\storage\\WHKVDSAF\\www.noom.com.html}
}

@inproceedings{ohMultimodalFoodJournaling2018,
  title = {Multimodal Food Journaling},
  booktitle = {Proceedings of the 3rd {{International Workshop}} on {{Multimedia}} for {{Personal Health}} and {{Health Care}}},
  author = {Oh, Hyungik and Nguyen, Jonathan and Soundararajan, Soundarya and Jain, Ramesh},
  year = {2018},
  pages = {39--47}
}

@article{ohPatientsWaitingCues2022,
  title = {Patients Waiting for Cues: {{Information}} Asymmetries and Challenges in Sharing Patient-Generated Data in the Clinic},
  shorttitle = {Patients {{Waiting}} for {{Cues}}},
  author = {Oh, Chi Young and Luo, Yuhan and St. Jean, Beth and Choe, Eun Kyoung},
  year = {2022},
  month = apr,
  journal = {Proceedings of the ACM on Human-Computer Interaction},
  volume = {6},
  number = {CSCW1},
  pages = {107:1--107:23},
  doi = {10.1145/3512954},
  urldate = {2022-12-08},
  abstract = {Patient-generated data (PGD) show great promise for informing the delivery of personalized and patient-centered care. However, patients' data tracking does not automatically lead to data sharing and discussion with clinicians, which can make it difficult to utilize and derive optimal benefit from PGD. In this paper, we investigate whether and how patients share their PGD with clinicians and the types of challenges that arise within this context. We describe patients' immediate experiences of PGD sharing with clinicians, based on our short onsite interviews with 57 patients who had just met with a clinician at a university health center. Our analyses identified overarching patterns in patients' PGD sharing practices and the associated challenges that arise from the information asymmetry between patients and clinicians and from patients' reliance on their memory to share their PGD. We discuss the implications of our findings for designing PGD-integrated health IT systems in ways to support patients' tracking of relevant PGD, clinicians' effective engagement with patients around PGD, and the efficient sharing and review of PGD within clinical settings.},
  keywords = {data tracking and sharing,doctor-patient communication,patient empowerment,patient-generated data,qualitative study},
  file = {C\:\\Users\\jseo1005\\Zotero\\storage\\YDR2CTXP\\Oh et al. - 2022 - Patients Waiting for Cues Information Asymmetries.pdf}
}

@article{ortegaDietaryAssessmentMethods2015,
  title = {Dietary Assessment Methods: {{Dietary}} Records},
  shorttitle = {Dietary Assessment Methods},
  author = {Ortega, Rosa M. and {P{\'e}rez-Rodrigo}, Carmen and {L{\'o}pez-Sobaler}, Ana M.},
  year = {2015},
  month = feb,
  journal = {Nutricion Hospitalaria},
  volume = {31 Suppl 3},
  pages = {38--45},
  issn = {1699-5198},
  doi = {10.3305/nh.2015.31.sup3.8749},
  abstract = {Dietary records or food diaries can be highlighted among dietary assessment methods of the current diet for their interest and validity. It is a prospective, open-ended survey method collecting data about the foods and beverages consumed over a previously specified period of time. Dietary records can be used to estimate current diet of individuals and population groups, as well as to identify groups at risk of inadequacy. It is a dietary assessment method interesting for its use in epidemiological or in clinical studies. High validity and precision has been reported for the method when used following adequate procedures and considering the sufficient number of days. Thus, dietary records are often considered as a reference method in validation studies. Nevertheless, the method is affected by error and has limitations due mainly to the tendency of subjects to report food consumption close to those socially desirable. Additional problems are related to the high burden posed on respondents. The method can also influence food behavior in respondents in order to simplify the registration of food intake and some subjects can experience difficulties in writing down the foods and beverages consumed or in describing the portion sizes. Increasing the number of days observed reduces the quality of completed diet records. It should also be considered the high cost of coding and processing information collected in diet records. One of the main advantages of the method is the registration of the foods and beverages as consumed, thus reducing the problem of food omissions due to memory failure. Weighted food records provide more precise estimates of consumed portions. New Technologies can be helpful to improve and ease collaboration of respondents, as well as precision of the estimates, although it would be desirable to evaluate the advantages and limitations in order to optimize the implementation.},
  langid = {english},
  pmid = {25719769},
  keywords = {Diet Records,Diet Surveys,Eating,Feeding Behavior,Humans,Reproducibility of Results}
}

@inproceedings{peckDataPersonalAttitudes2019,
  title = {Data Is {{Personal}}: {{Attitudes}} and {{Perceptions}} of {{Data Visualization}} in {{Rural Pennsylvania}}},
  shorttitle = {Data Is {{Personal}}},
  booktitle = {Proceedings of the 2019 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Peck, Evan M. and Ayuso, Sofia E. and {El-Etr}, Omar},
  year = {2019},
  month = may,
  series = {{{CHI}} '19},
  pages = {1--12},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3290605.3300474},
  urldate = {2022-12-07},
  abstract = {Many of the guidelines that inform how designers create data visualizations originate in studies that unintentionally exclude populations that are most likely to be among the 'data poor'. In this paper, we explore which factors may drive attention and trust in rural populations with diverse economic and educational backgrounds - a segment that is largely underrepresented in the data visualization literature. In 42 semi-structured interviews in rural Pennsylvania (USA), we find that a complex set of factors intermix to inform attitudes and perceptions about data visualization - including educational background, political affiliation, and personal experience. The data and materials for this research can be found at https://osf.io/uxwts/},
  isbn = {978-1-4503-5970-2},
  keywords = {data,information literacy,information visualization,rural}
}

@inproceedings{peckDataPersonalAttitudes2019a,
  title = {Data Is Personal: {{Attitudes}} and Perceptions of Data Visualization in Rural {{Pennsylvania}}},
  shorttitle = {Data Is {{Personal}}},
  booktitle = {Proceedings of the 2019 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Peck, Evan M. and Ayuso, Sofia E. and {El-Etr}, Omar},
  year = {2019},
  month = may,
  series = {{{CHI}} '19},
  pages = {1--12},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3290605.3300474},
  urldate = {2022-12-07},
  abstract = {Many of the guidelines that inform how designers create data visualizations originate in studies that unintentionally exclude populations that are most likely to be among the 'data poor'. In this paper, we explore which factors may drive attention and trust in rural populations with diverse economic and educational backgrounds - a segment that is largely underrepresented in the data visualization literature. In 42 semi-structured interviews in rural Pennsylvania (USA), we find that a complex set of factors intermix to inform attitudes and perceptions about data visualization - including educational background, political affiliation, and personal experience. The data and materials for this research can be found at https://osf.io/uxwts/},
  isbn = {978-1-4503-5970-2},
  keywords = {data,information literacy,information visualization,rural},
  file = {C\:\\Users\\jseo1005\\Zotero\\storage\\7GCK2ALA\\Peck et al. - 2019 - Data is Personal Attitudes and Perceptions of Dat.pdf}
}

@inproceedings{potluriPSSTEnablingBlind2022,
  title = {{{PSST}}: {{Enabling}} Blind or Visually Impaired Developers to Author Sonifications of Streaming Sensor Data},
  shorttitle = {{{PSST}}},
  booktitle = {Proceedings of the 35th {{Annual ACM Symposium}} on {{User Interface Software}} and {{Technology}}},
  author = {Potluri, Venkatesh and Thompson, John and Devine, James and Lee, Bongshin and Morsi, Nora and De Halleux, Peli and Hodges, Steve and Mankoff, Jennifer},
  year = {2022},
  month = oct,
  series = {{{UIST}} '22},
  pages = {1--13},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3526113.3545700},
  urldate = {2022-12-13},
  abstract = {We present the first toolkit that equips blind and visually impaired (BVI) developers with the tools to create accessible data displays. Called PSST (Physical computing Streaming Sensor data Toolkit), it enables BVI developers to understand the data generated by sensors from a mouse to a micro:bit physical computing platform. By assuming visual abilities, earlier efforts to make physical computing accessible fail to address the need for BVI developers to access sensor data. PSST enables BVI developers to understand real-time, real-world sensor data by providing control over what should be displayed, as well as when to display and how to display sensor data. PSST supports filtering based on raw or calculated values, highlighting, and transformation of data. Output formats include tonal sonification, nonspeech audio files, speech, and SVGs for laser cutting. We validate PSST through a series of demonstrations and a user study with BVI developers.},
  isbn = {978-1-4503-9320-1},
  keywords = {Accessibility,Blind or Visually Impaired (BVI) Programmers,Physical Computing,Toolkit},
  file = {C\:\\Users\\jseo1005\\Zotero\\storage\\M9RUY7PW\\Potluri et al. - 2022 - PSST Enabling Blind or Visually Impaired Develope.pdf}
}

@inproceedings{rahmanUnintrusiveEatingRecognition2015,
  title = {Unintrusive Eating Recognition Using {{Google Glass}}},
  booktitle = {2015 9th {{International Conference}} on {{Pervasive Computing Technologies}} for {{Healthcare}} ({{PervasiveHealth}})},
  author = {Rahman, Shah Atiqur and Merck, Christopher and Huang, Yuxiao and Kleinberg, Samantha},
  year = {2015},
  pages = {108--111},
  publisher = {{IEEE}},
  isbn = {1-63190-045-5}
}

@inproceedings{rossEpidemiologyFrameworkLargeScale2017,
  title = {Epidemiology as a {{Framework}} for {{Large-Scale Mobile Application Accessibility Assessment}}},
  booktitle = {Proceedings of the 19th {{International ACM SIGACCESS Conference}} on {{Computers}} and {{Accessibility}}},
  author = {Ross, Anne Spencer and Zhang, Xiaoyi and Fogarty, James and Wobbrock, Jacob O.},
  year = {2017},
  month = oct,
  pages = {2--11},
  publisher = {{ACM}},
  address = {{Baltimore Maryland USA}},
  doi = {10.1145/3132525.3132547},
  urldate = {2022-02-20},
  abstract = {Mobile accessibility is often a property considered at the level of a single mobile application (app), but rarely on a larger scale of the entire app ``ecosystem,'' such as all apps in an app store, their companies, developers, and user influences. We present a novel conceptual framework for the accessibility of mobile apps inspired by epidemiology. It considers apps within their ecosystems, over time, and at a population level. Under this metaphor, ``inaccessibility'' is a set of diseases that can be viewed through an epidemiological lens. Accordingly, our framework puts forth notions like risk and protective factors, prevalence, and health indicators found within a population of apps. This new framing offers terminology, motivation, and techniques to reframe how we approach and measure app accessibility. It establishes how app accessibility can benefit from multi-factor, longitudinal, and population-based analyses. Our epidemiology-inspired conceptual framework is the main contribution of this work, intended to provoke thought and inspire new work enhancing app accessibility at a systemic level. In a preliminary exercising of our framework, we perform an analysis of the prevalence of common determinants or accessibility barriers. We assess the health of a stratified sample of 100 popular Android apps using Google's Accessibility Scanner. We find that 100\% of apps have at least one of nine accessibility errors and examine which errors are most common. A preliminary analysis of the frequency of co-occurrences of multiple errors in a single app is also presented. We find 72\% of apps have five or six errors, suggesting an interaction among different errors or an underlying influence.},
  isbn = {978-1-4503-4926-0},
  langid = {english},
  file = {C\:\\Users\\jseo1005\\Zotero\\storage\\QK6JRDSA\\Ross et al. - 2017 - Epidemiology as a Framework for Large-Scale Mobile.pdf}
}

@article{sandersCocreationNewLandscapes2008,
  title = {Co-Creation and the New Landscapes of Design},
  author = {Sanders, Elizabeth B.-N. and Stappers, Pieter Jan},
  year = {2008},
  month = mar,
  journal = {CoDesign},
  volume = {4},
  number = {1},
  pages = {5--18},
  publisher = {{Taylor \& Francis}},
  issn = {1571-0882},
  doi = {10.1080/15710880701875068},
  urldate = {2022-12-07},
  abstract = {Designers have been moving increasingly closer to the future users of what they design and the next new thing in the changing landscape of design research has become co-designing with your users. But co-designing is actually not new at all, having taken distinctly different paths in the US and in Europe. The evolution in design research from a user-centred approach to co-designing is changing the roles of the designer, the researcher and the person formerly known as the `user'. The implications of this shift for the education of designers and researchers are enormous. The evolution in design research from a user-centred approach to co-designing is changing the landscape of design practice as well, creating new domains of collective creativity. It is hoped that this evolution will support a transformation toward more sustainable ways of living in the future.},
  keywords = {co-creation,co-design,collective creativity,design research,participatory design,user-centred design},
  file = {C\:\\Users\\jseo1005\\Zotero\\storage\\RDAEG4S8\\Sanders and Stappers - 2008 - Co-creation and the new landscapes of design.pdf}
}

@misc{Section508Gov,
  title = {Section508.Gov},
  urldate = {2022-12-14},
  abstract = {Information about the Section508.gov website, GSA's Government-wide IT Accessibility Team, and guidance to Federal agencies on accessible information and communication technology (ICT).},
  howpublished = {https://www.section508.gov/manage/laws-and-policies/},
  langid = {english},
  file = {C\:\\Users\\jseo1005\\Zotero\\storage\\V4KKZ6LF\\laws-and-policies.html}
}

@article{senjamSmartphonesAssistiveTechnology2021,
  title = {Smartphones as Assistive Technology for Visual Impairment},
  author = {Senjam, Suraj Singh},
  year = {2021},
  journal = {Eye},
  volume = {35},
  number = {8},
  pages = {2078--2080},
  publisher = {{Nature Publishing Group}},
  isbn = {1476-5454}
}

@inproceedings{sharifShouldSayDisabled2022,
  title = {Should {{I}} Say ``Disabled People'' or ``People with Disabilities''? {{Language}} Preferences of Disabled People between Identity- and Person-First Language},
  shorttitle = {Should {{I Say}} ``{{Disabled People}}'' or ``{{People}} with {{Disabilities}}''?},
  booktitle = {The 24th {{International ACM SIGACCESS Conference}} on {{Computers}} and {{Accessibility}}},
  author = {Sharif, Ather and McCall, Aedan Liam and Bolante, Kianna Roces},
  year = {2022},
  month = oct,
  pages = {1--18},
  publisher = {{ACM}},
  address = {{Athens Greece}},
  doi = {10.1145/3517428.3544813},
  urldate = {2022-12-04},
  isbn = {978-1-4503-9258-7},
  langid = {english},
  file = {C\:\\Users\\jseo1005\\Zotero\\storage\\2M93YUWN\\Sharif et al. - 2022 - Should I Say “Disabled People” or “People with Dis.pdf}
}

@misc{socialsecurityadministrationMeaningBlindnessDefined,
  title = {Meaning of Blindness as Defined in the Law.},
  author = {Social Security Administration, {\relax ORDP}},
  urldate = {2022-12-04},
  abstract = {Meaning of blindness as defined in the law.},
  howpublished = {https://www.ssa.gov/OP\_Home/cfr20/404/404-1581.htm},
  langid = {english},
  file = {C\:\\Users\\jseo1005\\Zotero\\storage\\D7DK7CBL\\404-1581.html}
}

@misc{stateofillinoisOfficeEqualEmployment,
  title = {Office for {{Equal Employment Opportunity}} and {{Affirmative Action}}},
  author = {{State of Illinois}},
  abstract = {The Office for Equal Employment Opportunity and Affirmative Action (EEO/AA) serves as a resource to all employees, departments, and customers of the Illinois Department of Revenue (IDOR) by facilitating compliance awareness and to promote an equitable working and service environment for all.},
  howpublished = {https://www2.illinois.gov/rev/aboutidor/EEO-AA/Pages/default.aspx}
}

@inproceedings{suhDevelopingValidatingUser2016,
  title = {Developing and Validating the User Burden Scale: {{A}} Tool for Assessing User Burden in Computing Systems},
  booktitle = {Proceedings of the 2016 {{CHI}} Conference on Human Factors in Computing Systems},
  author = {Suh, Hyewon and Shahriaree, Nina and Hekler, Eric B. and Kientz, Julie A.},
  year = {2016},
  pages = {3988--3999}
}

@article{thompsonUseMHealthTechnologies2019,
  title = {Use of {{mHealth Technologies}} by {{People}} with {{Vision Impairment}}},
  author = {Thompson, Nicole A and Morris, John T and Jones, Michael L},
  year = {2019},
  pages = {16},
  abstract = {This article presents analysis of qualitative data from two focus groups, one each with individuals with blindness and low vision, respectively, on use of mHealth technologies. The use of mHealth \textendash{} medical and public health practice supported by mobile devices, such as mobile phones, patient monitoring devices, personal digital assistants, and other wireless devices \textendash{} has expanded considerably in recent years and is expected to grow further. mHealth offers potential to reduce health disparities between people with disabilities and the general population by facilitating interaction with healthcare professionals and enhancing by supporting personal engagement in health data collection, goal setting and healthy living. People with disabilities \textendash{} in this case, people with severe visual impairment \textendash may benefit from access to these mHealth technologies and services. This exploratory qualitative research identifies patterns of use, barriers and facilitators, and attitudes to using mHealth. Participants generally did not immediately understand what the term mHealth referred to, but used or had familiarity with mHealth technologies like patient portals, mHealth mobile apps, health monitoring devices (e.g., glucose monitors), and wearable trackers. Participants expressed interest and, in some cases, strong positive affect for using mHealth technologies.},
  langid = {english},
  file = {C\:\\Users\\jseo1005\\Zotero\\storage\\FFIMRSV9\\Thompson et al. - 2019 - Use of mHealth Technologies by People with Vision .pdf}
}

@article{thompsonUseMHealthTechnologies2019a,
  title = {Use of {{mHealth}} Technologies by People with Vision Impairment},
  author = {Thompson, Nicole A. and Morris, John T. and Jones, Michael L. and DeRuyter, Frank},
  year = {2019},
  publisher = {{California State University, Northridge.}},
  issn = {2330-4219},
  urldate = {2022-12-12},
  abstract = {This article presents analysis of qualitative data from two focus groups, one each with individuals with blindness and low vision, respectively, on use of mHealth technologies. The use of mHealth -- medical and public health practice supported by mobile devices, such as mobile phones, patient monitoring devices, personal digital assistants, and other wireless devices -- has expanded considerably in recent years and is expected to grow further. mHealth offers potential to reduce health disparities between people with disabilities and the general population by facilitating interaction with healthcare professionals and enhancing by supporting personal engagement in health data collection, goal setting and healthy living. People with disabilities -- in this case, people with severe visual impairment -- may benefit from access to these mHealth technologies and services. This exploratory qualitative research identifies patterns of use, barriers and facilitators, and attitudes to using mHealth. Participants generally did not immediately understand what the term mHealth referred to, but used or had familiarity with mHealth technologies like patient portals, mHealth mobile apps, health monitoring devices (e.g., glucose monitors), and wearable trackers. Participants expressed interest and, in some cases, strong positive affect for using mHealth technologies.},
  langid = {english},
  annotation = {Accepted: 2019-05-28T21:25:54Z},
  file = {C\:\\Users\\jseo1005\\Zotero\\storage\\5EY8IRCY\\Thompson et al. - 2019 - Use of mHealth Technologies by People with Vision .pdf}
}

@inproceedings{troncosoaldasAIGuideAugmentedReality2020,
  title = {{{AIGuide}}: {{An}} Augmented Reality Hand Guidance Application for People with Visual Impairments},
  shorttitle = {{{AIGuide}}},
  booktitle = {Proceedings of the 22nd {{International ACM SIGACCESS Conference}} on {{Computers}} and {{Accessibility}}},
  author = {Troncoso Aldas, Nelson Daniel and Lee, Sooyeon and Lee, Chonghan and Rosson, Mary Beth and Carroll, John M. and Narayanan, Vijaykrishnan},
  year = {2020},
  month = oct,
  series = {{{ASSETS}} '20},
  pages = {1--13},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3373625.3417028},
  urldate = {2022-12-13},
  abstract = {Locating and grasping objects is a critical task in people's daily lives. For people with visual impairments, this task can be a daily struggle. The support of augmented reality frameworks in smartphones has the potential to overcome the limitations of current object detection applications designed for people with visual impairments. We present AIGuide, a self-contained offline smartphone application that leverages augmented reality technology to help users locate and pick up objects around them. We conducted a user study to validate its effectiveness at providing guidance, compare it to other assistive technology form factors, evaluate the use of multimodal feedback, and provide feedback about the overall experience. Our results show that AIGuide is a promising technology to help people with visual impairments locate and acquire objects in their daily routine.},
  isbn = {978-1-4503-7103-2},
  keywords = {assistive technology,augmented reality,mixed reality,mobile computing},
  file = {C\:\\Users\\jseo1005\\Zotero\\storage\\KL33SSV5\\Troncoso Aldas et al. - 2020 - AIGuide An Augmented Reality Hand Guidance Applic.pdf}
}

@article{turner-mcgrievyComparisonTraditionalMobile2013,
  title = {Comparison of Traditional versus Mobile App Self-Monitoring of Physical Activity and Dietary Intake among Overweight Adults Participating in an {{mHealth}} Weight Loss Program},
  author = {{Turner-McGrievy}, Gabrielle M. and Beets, Michael W. and Moore, Justin B. and Kaczynski, Andrew T. and {Barr-Anderson}, Daheia J. and Tate, Deborah F.},
  year = {2013},
  month = may,
  journal = {Journal of the American Medical Informatics Association: JAMIA},
  volume = {20},
  number = {3},
  pages = {513--518},
  issn = {1527-974X},
  doi = {10.1136/amiajnl-2012-001510},
  abstract = {OBJECTIVE: Self-monitoring of physical activity (PA) and diet are key components of behavioral weight loss programs. The purpose of this study was to assess the relationship between diet (mobile app, website, or paper journal) and PA (mobile app vs no mobile app) self-monitoring and dietary and PA behaviors. MATERIALS AND METHODS: This study is a post hoc analysis of a 6-month randomized weight loss trial among 96 overweight men and women (body mass index (BMI) 25-45 kg/m(2)) conducted from 2010 to 2011. Participants in both randomized groups were collapsed and categorized by their chosen self-monitoring method for diet and PA. All participants received a behavioral weight loss intervention delivered via podcast and were encouraged to self-monitor dietary intake and PA. RESULTS: Adjusting for randomized group and demographics, PA app users self-monitored exercise more frequently over the 6-month study (2.6{$\pm$}0.5 days/week) and reported greater intentional PA (196.4{$\pm$}45.9 kcal/day) than non-app users (1.2{$\pm$}0.5 days/week PA self-monitoring, p{$<$}0.01; 100.9{$\pm$}45.1 kcal/day intentional PA, p=0.02). PA app users also had a significantly lower BMI at 6 months (31.5{$\pm$}0.5 kg/m(2)) than non-users (32.5{$\pm$}0.5 kg/m(2); p=0.02). Frequency of self-monitoring did not differ by diet self-monitoring method (p=0.63); however, app users consumed less energy (1437{$\pm$}188 kcal/day) than paper journal users (2049{$\pm$}175 kcal/day; p=0.01) at 6 months. BMI did not differ among the three diet monitoring methods (p=0.20). CONCLUSIONS: These findings point to potential benefits of mobile monitoring methods during behavioral weight loss trials. Future studies should examine ways to predict which self-monitoring method works best for an individual to increase adherence.},
  langid = {english},
  pmcid = {PMC3628067},
  pmid = {23429637},
  keywords = {Adolescent,Adult,Cell Phone,Diet,Diet Records,Energy Intake,Exercise,Female,Humans,Internet,Male,Medical Informatics Applications,Middle Aged,Overweight,Self Care,Weight Reduction Programs,Young Adult},
  file = {C\:\\Users\\jseo1005\\Zotero\\storage\\HJKA9DYC\\Turner-McGrievy et al. - 2013 - Comparison of traditional versus mobile app self-m.pdf}
}

@misc{u.s.departmentofagricultureMyPlate,
  title = {{{MyPlate}}},
  author = {{U.S. Department of Agriculture}},
  urldate = {2022-12-12},
  howpublished = {https://www.myplate.gov/},
  file = {C\:\\Users\\jseo1005\\Zotero\\storage\\5IYXLGTZ\\www.myplate.gov.html}
}

@misc{universityofillinoisReasonableAccommodationsAmericans,
  title = {Reasonable {{Accommodations Under}} the {{Americans}} with {{Disabilities Act}}},
  author = {{University of Illinois}},
  abstract = {The University of Illinois is committed to creating an equitable and inclusive environment and recognizes individuals with disabilities as a valued component of diversity and of our campus community.},
  howpublished = {https://cam.illinois.edu/policies/hr-67/}
}

@misc{universityofillinoisSenateResolutionDiversity,
  title = {Senate {{Resolution}} on {{Diversity Values Statement}}},
  author = {{University of Illinois}},
  abstract = {EQ.13.01 Resolution on Diversity Values Statement University of Illinois Urbana-Champaign Senate},
  howpublished = {https://diversity.illinois.edu/about/senate-diversity-resolution}
}

@misc{universityofmarylandhumanresourcesAffirmativeActionEqual,
  title = {Affirmative {{Action}} and {{Equal Employment Opportunity}}},
  author = {{University of Maryland Human Resources}},
  abstract = {The University of Maryland, College Park (UMD) is proud to be an affirmative action and equal employment opportunity employer. This page shares information about the data we collect and use to construct our annual affirmative action plans (AAPs).},
  howpublished = {https://epress.trincoll.edu/webwriting/chapter/how-to-zotero/}
}

@article{vandelanottePresentFutureEHealth2016,
  title = {Past, Present, and Future of {{eHealth}} and {{mHealth}} Research to Improve Physical Activity and Dietary Behaviors},
  author = {Vandelanotte, Corneel and M{\"u}ller, Andre M. and Short, Camille E. and Hingle, Melanie and Nathan, Nicole and Williams, Susan L. and Lopez, Michael L. and Parekh, Sanjoti and Maher, Carol A.},
  year = {2016},
  journal = {Journal of nutrition education and behavior},
  volume = {48},
  number = {3},
  pages = {219-228. e1},
  publisher = {{Elsevier}},
  isbn = {1499-4046}
}

@article{vankesselDigitalHealthParadox2022,
  title = {Digital Health Paradox: {{International}} Policy Perspectives to Address Increased Health Inequalities for People Living with Disabilities},
  author = {{van Kessel}, Robin and O'Nuallain, Ella and Weir, Elizabeth and Wong, Brian Li Han and Anderson, Michael and {Baron-Cohen}, Simon and Mossialos, Elias},
  year = {2022},
  journal = {Journal of medical Internet research},
  volume = {24},
  number = {2},
  pages = {e33819},
  publisher = {{JMIR Publications Inc., Toronto, Canada}}
}

@article{vazquezAssistedPhotographyFramework2014,
  title = {An {{Assisted Photography Framework}} to {{Help Visually Impaired Users Properly Aim}} a {{Camera}}},
  author = {V{\'a}zquez, Marynel and Steinfeld, Aaron},
  year = {2014},
  month = nov,
  journal = {ACM Transactions on Computer-Human Interaction},
  volume = {21},
  number = {5},
  pages = {25:1--25:29},
  issn = {1073-0516},
  doi = {10.1145/2651380},
  urldate = {2022-12-15},
  abstract = {We propose an assisted photography framework to help visually impaired users properly aim a camera and evaluate our implementation in the context of documenting public transportation accessibility. Our framework integrates user interaction during the image capturing process to help users take better pictures in real time. We use an image composition model to evaluate picture quality and suggest providing audiovisual feedback to improve users' aiming position. With our particular framework implementation, blind participants were able to take pictures of similar quality to those taken by low vision participants without assistance. Likewise, our system helped low vision participants take pictures as good as those taken by fully sighted users. Our results also show a positive trend in favor of spoken directions to assist visually impaired users in comparison to tone and silent feedback. Positive usefulness ratings provided by full vision users further suggest that assisted photography has universal appeal.},
  keywords = {accessibility,Photography,transit,universal design,visually impaired},
  file = {C\:\\Users\\jseo1005\\Zotero\\storage\\UQW7MRJN\\Vázquez and Steinfeld - 2014 - An Assisted Photography Framework to Help Visually.pdf}
}

@inproceedings{vazquezHelpingVisuallyImpaired2012,
  title = {Helping Visually Impaired Users Properly Aim a Camera},
  booktitle = {Proceedings of the 14th International {{ACM SIGACCESS}} Conference on {{Computers}} and Accessibility},
  author = {V{\'a}zquez, Marynel and Steinfeld, Aaron},
  year = {2012},
  month = oct,
  series = {{{ASSETS}} '12},
  pages = {95--102},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2384916.2384934},
  urldate = {2022-12-13},
  abstract = {We evaluate three interaction modes to assist visually impaired users during the camera aiming process: speech, tone, and silent feedback. Our main assumption is that users are able to spatially localize what they want to photograph, and roughly aim the camera in the appropriate direction. Thus, small camera motions are sufficient for obtaining a good composition. Results in the context of documenting accessibility barriers related to public transportation show that audio feedback is valuable. Visually impaired users were not affected by audio feedback in terms of social comfort. Furthermore, we observed trends in favor of speech over tone, including higher ratings for ease of use. This study reinforces earlier work that suggests users who are blind or low vision find assisted photography appealing and useful.},
  isbn = {978-1-4503-1321-6},
  keywords = {accessibility,photography,transit,visually impaired},
  file = {C\:\\Users\\jseo1005\\Zotero\\storage\\GU8CBLWM\\Vázquez and Steinfeld - 2012 - Helping visually impaired users properly aim a cam.pdf}
}

@misc{wadernSchlossDagstuhlInclusive,
  title = {{Schloss Dagstuhl : Inclusive Data Visualization}},
  shorttitle = {{Schloss Dagstuhl}},
  author = {Wadern, 66687, Schloss Dagstuhl-Leibniz-Zentrum f{\"u}r Informatik GmbH},
  urldate = {2022-12-14},
  abstract = {Schloss Dagstuhl - Leibniz-Zentrum f\"ur Informatik GmbH (LZI), Wadern},
  howpublished = {https://www.dagstuhl.de/de/programm/kalender/semhp/?semnr=23252},
  langid = {ngerman},
  file = {C\:\\Users\\jseo1005\\Zotero\\storage\\3ZLP9BNT\\semhp.html}
}

@misc{WebAIMScreenReader2021,
  title = {{{WebAIM}}: {{Screen Reader User Survey}} \#9 {{Results}}},
  year = {2021},
  urldate = {2022-12-08},
  howpublished = {https://webaim.org/projects/screenreadersurvey9/},
  file = {C\:\\Users\\jseo1005\\Zotero\\storage\\TJLEP9SK\\screenreadersurvey9.html}
}

@inproceedings{whiteEasySnapRealtimeAudio2010,
  title = {{{EasySnap}}: Real-Time Audio Feedback for Blind Photography},
  shorttitle = {{{EasySnap}}},
  booktitle = {Adjunct Proceedings of the 23nd Annual {{ACM}} Symposium on {{User}} Interface Software and Technology},
  author = {White, Samuel and Ji, Hanjie and Bigham, Jeffrey P.},
  year = {2010},
  month = oct,
  series = {{{UIST}} '10},
  pages = {409--410},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/1866218.1866244},
  urldate = {2022-12-15},
  abstract = {This demonstration presents EasySnap, an application that enables blind and low-vision users to take high-quality photos by providing real-time audio feedback as they point their existing camera phones. Users can readily follow the audio instructions to adjust their framing, zoom level and subject lighting appropriately. Real-time feedback is achieved on current hardware using computer vision in conjunction with use patterns drawn from current blind photographers.},
  isbn = {978-1-4503-0462-7},
  keywords = {blind users,non-visual interfaces,photography},
  file = {C\:\\Users\\jseo1005\\Zotero\\storage\\6EEFSS7W\\White et al. - 2010 - EasySnap real-time audio feedback for blind photo.pdf}
}

@article{willettReproducibilityValiditySemiquantitative1985,
  title = {Reproducibility and Validity of a Semiquantitative Food Frequency Questionnaire},
  author = {Willett, W. C. and Sampson, L. and Stampfer, M. J. and Rosner, B. and Bain, C. and Witschi, J. and Hennekens, C. H. and Speizer, F. E.},
  year = {1985},
  month = jul,
  journal = {American Journal of Epidemiology},
  volume = {122},
  number = {1},
  pages = {51--65},
  issn = {0002-9262},
  doi = {10.1093/oxfordjournals.aje.a114086},
  abstract = {The aim of this study was to evaluate the reproducibility and validity of a 61-item semiquantitative food frequency questionnaire used in a large prospective study among women. This form was administered twice to 173 participants at an interval of approximately one year (1980-1981), and four one-week diet records for each subject were collected during that period. Intraclass correlation coefficients for nutrient intakes estimated by the one-week diet records (range = 0.41 for total vitamin A without supplements to 0.79 for vitamin B6 with supplements) were similar to those computed from the questionnaire (range = 0.49 for total vitamin A without supplements to 0.71 for sucrose), indicating that these methods were generally comparable with respect to reproducibility. With the exception of sucrose and total carbohydrate, nutrient intakes from the diet records tended to correlate more strongly with those computed from the questionnaire after adjustment for total caloric intake. Correlation coefficients between the mean calorie-adjusted intakes from the four one-week diet records and those from the questionnaire completed after the diet records ranged from 0.36 for vitamin A without supplements to 0.75 for vitamin C with supplements. Overall, 48\% of subjects in the lowest quintile of calorie-adjusted intake computed from the diet records were also in the lowest questionnaire quintile, and 74\% were in the lowest one of two questionnaire quintiles. Similarly, 49\% of those in the highest diet record quintile were also in the highest questionnaire quintile, and 77\% were in the highest one or two questionnaire quintiles. These data indicate that a simple self-administered dietary questionnaire can provide useful information about individual nutrient intakes over a one-year period.},
  langid = {english},
  pmid = {4014201},
  keywords = {Adult,Diet,Diet Surveys,Energy Intake,Female,Food Analysis,Humans,Middle Aged,Prospective Studies,Research Design,Surveys and Questionnaires}
}

@article{yanCurrentStatusAccessibility2019,
  title = {The {{Current Status}} of {{Accessibility}} in {{Mobile Apps}}},
  author = {Yan, Shunguo and Ramachandran, P. G.},
  year = {2019},
  month = feb,
  journal = {ACM Transactions on Accessible Computing},
  volume = {12},
  number = {1},
  pages = {1--31},
  issn = {1936-7228, 1936-7236},
  doi = {10.1145/3300176},
  urldate = {2022-02-20},
  abstract = {This study evaluated the status of accessibility in mobile apps by investigating the graphical user interface (GUI) structures and conformance to accessibility guidelines of 479 Android apps in 23 business categories from Google Play. An automated tool, IBM Mobile Accessibility Checker (MAC), was used to identify the accessibility issues, which were categorized as a violation (V), potential violation (PV), or warning (W). The results showed 94.8\%, 97.5\%, and 66.4\% of apps studied contained issues related to V, PV, or W, respectively. Five widget categories (TextView, ImageView, View, Button, and ImageButton) were used to create 92\% of the total number of the GUI elements and caused 89\%, 78\%, and 86\% of V, PV, and W, respectively. These accessibility issues were mainly caused by lack of element focus, missing element description, low text color contrast, lack of sufficient spacing between elements, and less than minimum sizes of text fonts and elements. Together, these accessibility issues accounted for 97.0\%, 77.8\%, and 94.5\% of V, PV, and W, respectively.             This study proposed coverage measures to estimate the percentage of accessibility issues identified by an automated tool. The result showed that MAC, on average, identified about 67\% of accessibility issues in mobile apps.             Two new accessibility conformance measures were proposed in this study: inaccessible element rate (IAER) and accessibility issue rate (AIR). IAER estimates the percentage of GUI elements that are inaccessible. AIR calculates the percentage of the actual number of accessibility issues relative to the maximum number of accessibility issues. Average IAER and AIR scores were 27.3\%, 19.9\%, 6.3\% and 20.7\%, 15.0\%, 5.4\% for V, PV, and W, respectively, for the studied apps. The IAER score showed approximately 30\% of the GUI elements had accessibility issues, and the AIR score indicated that 15\% of the accessibility issues remained and need to be fixed to make the apps accessible.},
  langid = {english},
  file = {C\:\\Users\\jseo1005\\Zotero\\storage\\QP6K4M54\\Yan and Ramachandran - 2019 - The Current Status of Accessibility in Mobile Apps.pdf}
}

@article{yanCurrentStatusAccessibility2019a,
  title = {The Current Status of Accessibility in Mobile Apps},
  author = {Yan, Shunguo and Ramachandran, P. G.},
  year = {2019},
  journal = {ACM Transactions on Accessible Computing (TACCESS)},
  volume = {12},
  number = {1},
  pages = {1--31},
  publisher = {{ACM New York, NY, USA}},
  isbn = {1936-7228}
}

@article{yuAccessibilityMHealthSelfCare2015,
  title = {Accessibility of {{mHealth Self-Care Apps}} for {{Individuals}} with {{Spina Bifida}}},
  author = {Yu, Daihua X. and Parmanto, Bambang and Dicianno, Brad E. and Pramana, Gede},
  year = {2015},
  month = apr,
  journal = {Perspectives in Health Information Management},
  volume = {12},
  number = {Spring},
  pages = {1h},
  issn = {1559-4122},
  urldate = {2022-02-20},
  abstract = {As the smartphone becomes ubiquitous, mobile health is becoming a viable technology to empower individuals to engage in preventive self-care. An innovative mobile health system called iMHere (Internet Mobile Health and Rehabilitation) has been developed at the University of Pittsburgh to support self-care and adherence to self-care regimens for individuals with spina bifida and other complex conditions who are vulnerable to secondary complications. The goal of this study was to explore the accessibility of iMHere apps for individuals with spina bifida. Six participants were asked to perform tasks in a lab environment. Though all of the participants were satisfied with the iMHere apps and would use them again in the future, their needs and preferences to access and use iMHere apps differed. Personalization that provides the ability for a participant to modify the appearance of content, such as the size of the icons and the color of text, could be an ideal solution to address potential issues and barriers to accessibility. The importance of personalization\textemdash and potential strategies\textemdash for accessibility are discussed.},
  pmcid = {PMC4696094},
  pmid = {26755902}
}

@inproceedings{zhaoFaceRecognitionApplication2018,
  title = {A {{Face Recognition Application}} for {{People}} with {{Visual Impairments}}: {{Understanding Use Beyond}} the {{Lab}}},
  shorttitle = {A {{Face Recognition Application}} for {{People}} with {{Visual Impairments}}},
  booktitle = {Proceedings of the 2018 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Zhao, Yuhang and Wu, Shaomei and Reynolds, Lindsay and Azenkot, Shiri},
  year = {2018},
  month = apr,
  series = {{{CHI}} '18},
  pages = {1--14},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3173574.3173789},
  urldate = {2022-12-15},
  abstract = {Recognizing others is a major challenge for people with visual impairments (VIPs) and can hinder engagement in social activities. We present Accessibility Bot, a research prototype bot on Facebook Messenger, that leverages state-of-the-art computer vision and a user's friends' tagged photos on Facebook to help people with visual impairments recognize their friends. Accessibility Bot provides users information about identity and facial expressions and attributes of friends captured by their phone's camera. To guide our design, we interviewed eight VIPs to understand their challenges and needs in social activities. After designing and implementing the bot, we conducted a diary study with six VIPs to study its use in everyday life. While most participants found the Bot helpful, their experience was undermined by perceived low recognition accuracy, difficulty aiming a camera, and lack of knowledge about the phone's status. We discuss these real-world challenges, identify suitable use cases for Accessibility Bot, and distill design implications for future face recognition applications.},
  isbn = {978-1-4503-5620-6},
  keywords = {face recognition,social activity,visual impairment},
  file = {C\:\\Users\\jseo1005\\Zotero\\storage\\IAAB5D9B\\Zhao et al. - 2018 - A Face Recognition Application for People with Vis.pdf}
}

@inproceedings{zhongRealTimeObject2013,
  title = {Real Time Object Scanning Using a Mobile Phone and Cloud-Based Visual Search Engine},
  booktitle = {Proceedings of the 15th {{International ACM SIGACCESS Conference}} on {{Computers}} and {{Accessibility}}},
  author = {Zhong, Yu and Garrigues, Pierre J. and Bigham, Jeffrey P.},
  year = {2013},
  month = oct,
  series = {{{ASSETS}} '13},
  pages = {1--8},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2513383.2513443},
  urldate = {2022-12-15},
  abstract = {Computer vision and human-powered services can provide blind people access to visual information in the world around them, but their efficacy is dependent on high-quality photo inputs. Blind people often have difficulty capturing the information necessary for these applications to work because they cannot see what they are taking a picture of. In this paper, we present Scan Search, a mobile application that offers a new way for blind people to take high-quality photos to support recognition tasks. To support realtime scanning of objects, we developed a key frame extraction algorithm that automatically retrieves high-quality frames from continuous camera video stream of mobile phones. Those key frames are streamed to a cloud-based recognition engine that identifies the most significant object inside the picture. This way, blind users can scan for objects of interest and hear potential results in real time. We also present a study exploring the tradeoffs in how many photos are sent, and conduct a user study with 8 blind participants that compares Scan Search with a standard photo-snapping interface. Our results show that Scan Search allows users to capture objects of interest more efficiently and is preferred by users to the standard interface.},
  isbn = {978-1-4503-2405-2},
  keywords = {accessibility,blind user,mobile,real time object scanning},
  file = {C\:\\Users\\jseo1005\\Zotero\\storage\\63CVCURT\\Zhong et al. - 2013 - Real time object scanning using a mobile phone and.pdf}
}
